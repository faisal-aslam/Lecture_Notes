\section{Introduction to Space Complexity}

So far, we have focused on \textbf{time complexity}. Today, we will explore \textbf{space complexity}, which measures the memory resources
an algorithm uses. Space complexity is important for two key reasons:

\begin{enumerate}
    \item It quantifies the \textbf{memory consumption} of an algorithm.
    \item If two algorithms have the \textbf{same time complexity}, but one uses more space, the latter may run slower due to higher memory overhead.
\end{enumerate}

We will only consider space complexity for \textbf{decidable Turing Machines (TMs)}, meaning they halt on all inputs. 
With this in mind, let's formally define space complexity.

\section{Space Complexity Definitions}

\subsection{1. Deterministic Turing Machines (DTM)}
The \textbf{space complexity} of a decidable deterministic TM is a function \( f: \mathbb{N} \rightarrow \mathbb{N} \), where \( f(n) \) is 
the \textbf{maximum number of tape cells visited} by the TM on any input of size \( n \).

\subsection{2. Non-Deterministic Turing Machines (NTM)}
The \textbf{space complexity} of a decidable non-deterministic TM is a function \( f: \mathbb{N} \rightarrow \mathbb{N} \), 
where \( f(n) \) is the \textbf{maximum number of tape cells visited} by the TM \textbf{along any computational branch} for any input of size \( n \).

\section{Clarifying the Notion of ``Visited'' in Space Complexity}

To properly analyze space complexity, we must precisely define what constitutes a ``visited'' tape cell:

\begin{definition}[Visited Cell]
A tape cell is considered \textbf{visited} if the Turing Machine's read/write head ever occupies or scans that cell during computation. Each distinct cell visited contributes exactly one unit to the space complexity, regardless of how many times it is accessed.
\end{definition}

\subsection{Contrast with Time Complexity}
\begin{itemize}
    \item \textbf{Space Complexity} counts only the \textit{number of distinct cells} ever accessed during computation. Multiple accesses to the same cell do not increase the space measure.

    \item \textbf{Time Complexity} counts the \textit{total number of transitions} executed by the TM. Each basic operation (read, write, move, state change) constitutes one transition, regardless of whether it involves previously accessed cells.

%    \item \textbf{Configuration vs Transition}: While a configuration (complete state snapshot including tape contents, head position, and control state) is useful for decidability proofs, time complexity specifically counts transitions between configurations.
\end{itemize}

\subsection{Key Implications}
\begin{itemize}
    \item Space complexity depends only on the \textit{maximum workspace} needed, not on how frequently cells are reused.

    \item Time complexity depends on the \textit{total computation length}, counting every transition, including repeated operations on the same cells.

    \item This distinction explains why some problems can have different space and time complexity classes (e.g., problems in $\text{PSPACE}$ but not $\text{P}$).
\end{itemize}

\begin{example}
Consider a TM that writes $n$ bits by repeatedly moving back-and-forth across $O(1)$ cells:
\begin{itemize}
    \item Its \textit{space complexity} is $O(1)$ (fixed number of cells)
    \item Its \textit{time complexity} is $\Omega(n)$ ($n$ transitions needed)
\end{itemize}
\end{example}


\section{Complexity Classes for Space}

We now define some fundamental classes of space complexity:

\subsection{SPACE(\( f(n) \))}
\[
\text{SPACE}(f(n)) = \{ B \mid \text{some deterministic 1-tape TM } M \text{ decides } B \text{ using } O(f(n)) \text{ space} \}
\]
A more precise name for this class is \textbf{DSPACE(\( f(n) \))}, but we will use the conventional notation.

\subsection{NSPACE(\( f(n) \))}
\[
\text{NSPACE}(f(n)) = \{ B \mid \text{some non-deterministic 1-tape TM } M \text{ decides } B \text{ using } O(f(n)) \text{ space} \}
\]

\subsection{PSPACE}
\[
\text{PSPACE} = \bigcup_{k \in \mathbb{N}} \text{SPACE}(n^k)
\]
Thus, PSPACE contains all languages decidable by a deterministic TM using \textbf{polynomial space}.

\subsection{NPSPACE}
\[
\text{NPSPACE} = \bigcup_{k \in \mathbb{N}} \text{NSPACE}(n^k)
\]
Thus, NPSPACE contains all languages decidable by a non-deterministic TM using \textbf{polynomial space}.

These classes help us categorize problems based on their \textbf{memory requirements} under deterministic and non-deterministic computation models.

\section{Space Complexity of Multitape Turing Machines}

\begin{definition}
For a $k$-tape Turing machine $M$, its \textbf{space complexity} is the function $f: \mathbb{N} \rightarrow \mathbb{N}$ where $f(n)$ represents the 
maximum total number of distinct cells visited across \emph{all} tapes during $M$'s computation on any input of length $n$. 
\end{definition}

\begin{theorem}[Multitape to Single-Tape Simulation]
    Any $k$-tape Turing machine (TM) with space complexity $O(s(n))$ can be simulated by a single-tape TM with the same space complexity, 
    i.e., $O(s(n))$, with the space only increased by a constant factor. 
    This constant factor depends only on the number of tapes $k$, and not on the input size $n$.
\end{theorem}

    
\section{Relationship Between Time and Space Complexity}

\begin{theorem}[Time-Space Containment]
    For any space-constructible function $t(n) \geq n$:
    \begin{enumerate}
        \item $\text{TIME}(t(n)) \subseteq \text{SPACE}(t(n))$ \\
        (A machine using $t(n)$ time can visit at most $t(n)$ tape cells, hence its space usage is bounded by $t(n)$.)
        
        \item $\text{SPACE}(t(n)) \subseteq \text{TIME}(2^{O(t(n))})$ \\
        (A machine using $t(n)$ space has at most $2^{O(t(n))}$ possible configurations, so it must halt within that many transitions if it halts at all.)
    
        \item Based on the first inclusion, we can conclude that:
        \begin{itemize}
            \item $P \subseteq PSPACE$
            \item $EXPTIME \subseteq EXPSPACE$
            \item $NP \subseteq NPSPACE$
        \end{itemize}
    
        \item Based on the second inclusion, we can conclude that:
        \begin{itemize}
            \item $PSPACE \subseteq EXPTIME$
            \item $NPSPACE \subseteq EXPTIME$ \\
            (Because by Savitch’s theorem, $NPSPACE = PSPACE$, so the same time bound applies.)
        \end{itemize}
    \end{enumerate}    
\end{theorem}

\subsection{Explanation of the Containments}

\begin{description}
    \item[Time Bounds Space:] 
    Each transition of a TM can access at most one new tape cell. Therefore, a machine that halts after $O(t(n))$ transitions cannot use more than $O(t(n))$ space:
    \[
    \text{TIME}(t(n)) \subseteq \text{SPACE}(t(n))
    \]
    
    \item[Space Bounds Time via Configurations:]
    For a TM using $O(t(n))$ space:
    \begin{itemize}
        \item Let $Q$ be the set of states ($|Q|$ constant)
        \item Let $\Gamma$ be the tape alphabet ($|\Gamma|$ constant)
        \item Number of head positions: $O(t(n))$
        \item Number of possible tape contents: $|\Gamma|^{O(t(n))} = 2^{O(t(n))}$
    \end{itemize}
    Thus, the total number of distinct configurations is:
    \[
    |Q| \times O(t(n)) \times 2^{O(t(n))} = 2^{O(t(n))}
    \]
    
    Since the machine must halt (by decidability), it cannot execute more transitions than its number of possible configurations (otherwise it would repeat a configuration and loop forever). Therefore:
    \[
    \text{SPACE}(t(n)) \subseteq \text{TIME}(2^{O(t(n))})
    \]
\end{description}

\subsection{Key Observations}
\begin{itemize}
    \item Each transition corresponds to one unit of time complexity
    \item The configuration count ($2^{O(t(n))}$) provides an upper bound on possible distinct transitions before halting
    \item This explains why $\text{PSPACE} \subseteq \text{EXP}$ (problems solvable in polynomial space can require exponential time)
\end{itemize}

\section{Relationship Between NP and PSPACE}

\begin{theorem}
    $NP \subseteq PSPACE$
\end{theorem}

\begin{proof}
The proof proceeds in the following steps:
\begin{enumerate}
    \item \textbf{Proof that $SAT \in PSPACE$:}  
    Although $SAT$ is an NP-complete problem, it can be decided using polynomial space by trying all possible truth assignments sequentially without storing all of them simultaneously. We can evaluate each assignment one-by-one using only polynomial space, because storing a single assignment and checking the formula can be done within space proportional to the input size.

    \item \textbf{Proof that if $A \le_p B$ and $B \in PSPACE$, then $A \in PSPACE$:}  
    Suppose $f$ is a polynomial-time reduction from $A$ to $B$, and we have a polynomial-space algorithm for $B$. To decide if $x \in A$, we compute $f(x)$ (which requires only polynomial time, and hence polynomial space), and then decide whether $f(x) \in B$ using the polynomial-space algorithm for $B$. Thus, $A$ can be decided in polynomial space.

    \item \textbf{Conclusion:}  
    Every language $L \in NP$ is polynomial-time reducible to $SAT$ (since $SAT$ is NP-complete). By steps 1 and 2, and since polynomial-time reductions preserve membership in $PSPACE$, we conclude that $L \in PSPACE$. Hence, $NP \subseteq PSPACE$.
\end{enumerate}
\end{proof}

\begin{theorem} $PSPACE = coPSPACE$\end{theorem}
\begin{proof}
    \subsection{Closure Under Complementation}

\begin{theorem}[PSPACE = coPSPACE]
The complexity class PSPACE is closed under complementation. That is:
\[
\text{PSPACE} = \text{coPSPACE}
\]
\end{theorem}

\begin{proof}[Proof Sketch]
For any language $L \in \text{PSPACE}$, let $M$ be a TM that decides $L$ using polynomial space. We can construct a machine $M'$ that decides $\overline{L}$ (the complement of $L$) as follows:

\begin{enumerate}
    \item $M'$ simulates $M$ using the same polynomial space bound $p(n)$
    \item When $M$ would accept, $M'$ rejects
    \item When $M$ would reject, $M'$ accepts
    \item $M'$ maintains the same space complexity since:
    \begin{itemize}
        \item It uses exactly the same tape cells as $M$
        \item The finite control only needs constant additional space to track the inverted acceptance condition
    \end{itemize}
\end{enumerate}

The key observations are:
\begin{itemize}
    \item Space-bounded computation can be \textit{deterministically} complemented without increasing space usage
    \item Unlike time complexity, we don't need to worry about "timing out" - the space bound guarantees termination
    \item This fails for nondeterministic space classes unless Savitch's theorem gives us determinization (which it does for polynomial space)
\end{itemize}
\end{proof}

\paragraph{Contrast with Other Complexity Classes}
\begin{itemize}
    \item \textbf{Time Classes:} $\text{P} = \text{coP}$ (by similar argument), but it remains unknown whether $\text{NP} = \text{coNP}$
    \item \textbf{Logarithmic Space:} $\text{L} = \text{coL}$ (by Szelepcsényi's theorem for NSPACE)
    \item \textbf{General Pattern:} Space-bounded classes tend to be closed under complementation, while time-bounded classes may not be
\end{itemize}

\paragraph{Significance}
This property demonstrates an important advantage of space-bounded computation:
\begin{itemize}
    \item Space resources can be \textit{reused} during computation
    \item There's no need to store computation history for complementation
    \item Contrast with time-bounded computation where complementation might require storing or recomputing information
\end{itemize}
\end{proof}

\begin{theorem}
    $coNP \subseteq PSPACE$
\end{theorem}

\begin{proof}
The proof proceeds as follows:
\begin{enumerate}
    \item \textbf{Fact:} It is known that $PSPACE = coPSPACE$, meaning the class of languages decidable in polynomial space is closed under 
    complementation.

    \item \textbf{Conclusion:}  
    Since $NP \subseteq PSPACE$, taking complements yields $coNP \subseteq coPSPACE = PSPACE$.
\end{enumerate}
\end{proof}

\section{Open Problems in Space Complexity}

Space complexity theory contains several fundamental unresolved questions. Below we present the most significant open problems with their current status.

Although we know that $\mathbf{NP}, \mathbf{coNP} \subseteq \mathbf{PSPACE}$, it is still unknown whether $\mathbf{NP} = \mathbf{coNP}$ or whether either of them equals $\mathbf{P}$. These inclusions help structure the complexity landscape, as illustrated in the diagram below.

\begin{center}
    \begin{tikzpicture}[scale=1.4]
      % PSPACE - largest container
      \draw[thick, rounded corners=12pt, fill=blue!5] (-3.2,-1.8) rectangle (3.2,1.8);
      \node at (0,2.1) {\large\textbf{PSPACE}};
      
      % NP and coNP - properly contained intersecting ellipses
      \begin{scope}
        \clip (-1.4,-0.6) ellipse (1.8cm and 1.1cm);
        \fill[green!8] (1.4,-0.6) ellipse (1.8cm and 1.1cm);
      \end{scope}
      \draw[thick] (-1.4,-0.6) ellipse (1.8cm and 1.1cm);
      \draw[thick] (1.4,-0.6) ellipse (1.8cm and 1.1cm);
      \node at (-2.1,-0.1) {\textbf{NP}};
      \node at (2.1,-0.1) {\textbf{coNP}};
      
      % P - smallest circle in the intersection
      \draw[thick, fill=white] (0,-0.6) circle (0.6cm);
      \node at (0,-0.6) {\textbf{P}};
      
      % Right-side text explanations
      \node[align=left, anchor=west] at (4,1.0) {$\bullet\ \mathbf{P} \subseteq \mathbf{NP} \subseteq \mathbf{PSPACE}$};
      \node[align=left, anchor=west] at (4,0.5) {$\bullet\ \mathbf{P} \subseteq \mathbf{coNP} \subseteq \mathbf{PSPACE}$};
      \node[align=left, anchor=west,red] at (4,0.0) {$\bullet\ \mathbf{NP} \stackrel{?}{=} \mathbf{coNP}$ (Open)};
      \node[align=left, anchor=west,red] at (4,-0.5) {$\bullet\ \mathbf{P} \stackrel{?}{=} \mathbf{NP}$ (Open)};
    \end{tikzpicture}
\end{center}

\subsection{Class Containment Problems}

\begin{itemize}
    \item \textbf{$\mathbf{P}$ vs $\mathbf{PSPACE}$}
    \begin{itemize}
        \item[\textit{Known}:] $\mathbf{P} \subseteq \mathbf{PSPACE}$ (via space-time hierarchy)
        \item[\textit{Open}:] Is the containment proper?
        \item[\textit{Significance}:] Would imply all polynomial-space algorithms can be made time-efficient
        \item[\textit{Conjecture}:] $\mathbf{P} \neq \mathbf{PSPACE}$
    \end{itemize}

    \item \textbf{$\mathbf{NP}$ vs $\mathbf{PSPACE}$}
    \begin{itemize}
        \item[\textit{Known}:] $\mathbf{NP} \subseteq \mathbf{PSPACE} \subseteq \mathbf{EXP}$
        \item[\textit{Open}:] Is $\mathbf{NP} = \mathbf{PSPACE}$?
        \item[\textit{Note}:] Resolution may require new proof techniques beyond relativization
    \end{itemize}

    \item \textbf{$\mathbf{L}$ vs $\mathbf{P}$}
    \begin{itemize}
        \item[\textit{Known}:] $\mathbf{L} \subseteq \mathbf{NL} \subseteq \mathbf{P}$
        \item[\textit{Open}:] Does $\mathbf{L} = \mathbf{P}$ hold?
        \item[\textit{Implications}:] Negative answer would confirm fundamental limits of space-restricted computation
    \end{itemize}
\end{itemize}

\subsection{Specific Separation Problems}

\begin{itemize}
    \item \textbf{Space Hierarchy Tightness}
    \begin{itemize}
        \item[\textit{Theorem}:] $\mathbf{SPACE}(o(f(n))) \subsetneq \mathbf{SPACE}(f(n))$ for space-constructible $f$
        \item[\textit{Open}:] Find natural problems in $\mathbf{SPACE}(n^2) \setminus \mathbf{SPACE}(n)$
        \item[\textit{Recent Progress}:] Limited for sub-polynomial separations
    \end{itemize}

    \item \textbf{$\mathbf{NL}$ vs $\mathbf{L}$}
    \begin{itemize}
        \item[\textit{Known}:] $\mathbf{NL} = \mathbf{coNL}$ (Immerman-Szelepcsényi)
        \item[\textit{Open}:] Does nondeterminism help in logspace?
        \item[\textit{Approaches}:] Current attempts focus on branching programs
    \end{itemize}
\end{itemize}

\subsection{Space-Time Tradeoffs}

\begin{itemize}
    \item \textbf{Fundamental Limits}
    \begin{itemize}
        \item[\textit{Key Question}:] Can SAT be solved in $n^{1+o(1)}$ space and $2^{n^{o(1)}}$ time?
        \item[\textit{Barriers}:] Current techniques cannot rule out mild exponential time
    \end{itemize}
\end{itemize}

\paragraph{Research Connections} These problems relate to:
\begin{itemize}
    \item Circuit complexity (e.g., $\mathbf{NC}$ vs $\mathbf{P}$)
    \item Descriptive complexity (logical characterizations)
    \item Pseudorandomness and derandomization
\end{itemize}

\section{The Complexity of TQBF}

\subsection{Basic Properties and PSPACE Membership}

\begin{definition}[TQBF]
\texttt{TQBF} (True Quantified Boolean Formula) is the set of fully quantified Boolean formulas of the form:
\[
\Phi = Q_1 x_1 \; Q_2 x_2 \; \dots \; Q_n x_n \; \phi(x_1, x_2, \dots, x_n)
\]
where:
\begin{itemize}
    \item Each $Q_i \in \{\exists, \forall\}$ is a quantifier,
    \item $\phi$ is a propositional Boolean formula over the variables $x_1, \dots, x_n$,
    \item Every variable appears within the scope of its quantifier.
\end{itemize}
The goal is to determine whether $\Phi$ evaluates to true.
\end{definition}

\begin{theorem}
\texttt{TQBF} $\in$ \texttt{PSPACE}.
\end{theorem}

\begin{proof}
We describe a recursive algorithm to evaluate a fully quantified Boolean formula:

\begin{enumerate}
    \item If $\Phi$ has no quantifiers, evaluate the propositional formula $\phi$ directly under the current variable assignment.
    \item Otherwise, let $\Phi = Qx \; \Psi$ where $Q \in \{\exists, \forall\}$ and $\Psi$ is the subformula.
    \begin{itemize}
        \item Recursively evaluate $\Psi$ with $x = 0$ and $x = 1$.
        \item If $Q = \exists$, return \texttt{true} if at least one of the two recursive calls returns \texttt{true}.
        \item If $Q = \forall$, return \texttt{true} only if both recursive calls return \texttt{true}.
    \end{itemize}
\end{enumerate}

\paragraph{Time vs Space Clarification:}
This algorithm makes up to $2^n$ recursive calls due to the binary branching at each quantifier. So the \textbf{time complexity is exponential} in $n$.

However, at any point in the execution, only \textbf{one branch of the recursion} is being explored. Hence, we only need to store:
\begin{itemize}
    \item the current variable assignment (one bit per variable),
    \item the current position in the recursion (call stack frame).
\end{itemize}

Thus:
\begin{itemize}
    \item The depth of the recursion is at most $n$ (one level per quantifier).
    \item Each call uses $O(\text{poly}(n))$ space to store the variable value and local data.
    \item Total space used is $O(n \cdot \text{poly}(n)) = \text{poly}(n)$.
\end{itemize}

\paragraph{Analogy for Students:} 
Think of a depth-first search in a binary tree of depth $n$: it visits $2^n$ nodes, but only needs $O(n)$ space because it explores only one path at a time. Similarly, our TQBF algorithm never holds the entire tree in memory — only the current path being evaluated.

\paragraph{Conclusion:}
The algorithm correctly determines the truth of any fully quantified Boolean formula using only polynomial space. Hence:
\[
\texttt{TQBF} \in \texttt{PSPACE}.
\]
\end{proof}

\begin{definition}[PSPACE-Completeness]
    A language $L$ is \textbf{PSPACE-complete} if:
    \begin{enumerate}
        \item $L \in \texttt{PSPACE}$, and
        \item For every language $A \in \texttt{PSPACE}$, there exists a polynomial-time computable function $f$ such that:
        \[
        x \in A \iff f(x) \in L.
        \]
    \end{enumerate}
    That is, every PSPACE problem reduces to $L$ under a polynomial-time reduction.
    \end{definition}
    
    \begin{theorem}
    \texttt{TQBF} is \texttt{PSPACE}-complete.
    \end{theorem}
    
    \begin{proof}
    We already proved that $\texttt{TQBF} \in \texttt{PSPACE}$.
    
    To show completeness, we need to show that any language $A \in \texttt{PSPACE}$ can be reduced to $\texttt{TQBF}$ in polynomial time.
    
    Let $M$ be a deterministic Turing machine that decides $A$ in polynomial space, say space $n^k$ on inputs of length $n$. The idea is to construct, given an input $x$, a fully quantified Boolean formula $\Phi_x$ that is true if and only if $M$ accepts $x$.
    
    \paragraph{High-Level Idea:}
    We encode the computation of $M$ on input $x$ as a sequence of configurations and construct a formula that checks whether there exists a valid accepting computation path. This technique is known as the \emph{configuration graph} approach.
    
    \paragraph{Key Steps:}
    \begin{itemize}
        \item A configuration consists of the current state, tape contents, and head position.
        \item Since $M$ uses at most $n^k$ space, the total number of configurations is at most $2^{O(n^k)}$.
        \item Construct a Boolean formula $\phi_{\text{reach}}(C_{\text{start}}, C_{\text{accept}}, t)$ that is true iff $C_{\text{accept}}$ is reachable from $C_{\text{start}}$ in $t$ steps.
        \item Use a recursive formula similar to Savitch’s Theorem:
        \[
        \text{Reach}(C_1, C_2, t) =
        \begin{cases}
            \text{true} & \text{if } t = 1 \text{ and } C_1 \vdash C_2 \\
            \exists C_m \; \big(\text{Reach}(C_1, C_m, t/2) \land \text{Reach}(C_m, C_2, t/2)\big) & \text{if } t > 1
        \end{cases}
        \]
        \item This recursion can be encoded into a quantified Boolean formula of polynomial size.
    \end{itemize}
    
    \paragraph{Result:}
    The formula $\Phi_x$ can be constructed in polynomial time, and it is true iff $M$ accepts $x$. Thus:
    \[
    x \in A \iff \Phi_x \in \texttt{TQBF}.
    \]
    
    \paragraph{Conclusion:}
    Every problem in \texttt{PSPACE} can be reduced to \texttt{TQBF} in polynomial time. Therefore:
    \[
    \texttt{TQBF} \text{ is } \texttt{PSPACE}\text{-complete}.
    \]
    \end{proof}
    
\subsection{Visualizing the Recursive Evaluation}

\begin{center}
\begin{tikzpicture}[
    level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2.5cm},
    every node/.style={draw, rounded corners, align=center}]
  
  \node {$\exists x$}
    child {
        node {$\forall y$}
        child {node[rectangle] {$(0 \lor 0)$\\$\rightarrow 0$}}
        child {node[rectangle] {$(0 \lor 1)$\\$\rightarrow 1$}}}
    child {
        node {$\forall y$}
        child {node[rectangle] {$(1 \lor 0)$\\$\rightarrow 1$}}
        child {node[rectangle] {$(1 \lor 1)$\\$\rightarrow 1$}}};
  
  \node[draw=none] at (0,-4.5) {
    Result: $\exists x\forall y(x\lor y)$ is true since right subtree satisfies $\forall y$
  };
\end{tikzpicture}
\end{center}

\subsection{Space Efficiency}

Key observations about the algorithm:
\begin{itemize}
    \item \textbf{Depth-first evaluation}: Only one path active at any time
    \item \textbf{Space reuse}: Each completed subtree's space is reclaimed
    \item \textbf{Constant overhead}: Per recursive call uses $O(1)$ space
    \item \textbf{Total space}: $O(n)$ for $n$ variables
\end{itemize}

This shows how \texttt{TQBF} can be in \texttt{PSPACE} despite the exponential recursion tree.


\section{Savitch's Theorem}

\begin{theorem}[Savitch's Theorem]
For any function $f(n) \geq \log n$ that is space-constructible,
\[
\texttt{NSPACE}(f(n)) \subseteq \texttt{DSPACE}(f(n)^2).
\]
\end{theorem}

\begin{remark}
Savitch's Theorem shows that nondeterministic space is not much more powerful than deterministic space. In particular, it implies that:
\[
\texttt{PSPACE} = \texttt{NPSPACE}.
\]
This is in stark contrast with time complexity, where it is widely conjectured that $\texttt{P} \ne \texttt{NP}$.
\end{remark}

\begin{proof}
We describe a deterministic algorithm that simulates a nondeterministic Turing machine (NTM) using only $O(f(n)^2)$ space.

Let $M$ be a nondeterministic Turing machine that operates in $f(n)$ space on input $x$ of length $n$. Let $C_1$ and $C_2$ be two configurations of $M$ on input $x$.

We define a recursive procedure:
\[
\texttt{REACH}(C_1, C_2, t)
\]
which returns true if there is a computation path from $C_1$ to $C_2$ in at most $t$ steps.

\paragraph{Base Case:} If $t = 1$, return true iff $C_2$ is directly reachable from $C_1$ in one step.

\paragraph{Recursive Case:} If $t > 1$, we guess a middle configuration $C_m$ and check:
\[
\texttt{REACH}(C_1, C_m, \lfloor t/2 \rfloor) \text{ and } \texttt{REACH}(C_m, C_2, \lceil t/2 \rceil)
\]
We try all possible $C_m$. The total number of configurations is at most $2^{O(f(n))}$ since each configuration uses at most $f(n)$ space.

\paragraph{Space Analysis:}
\begin{itemize}
    \item Each recursive call stores $C_1$, $C_2$, $t$, and the guessed $C_m$, requiring $O(f(n))$ space per call.
    \item The recursion depth is $O(\log t)$, and $t \leq 2^{O(f(n))}$.
    \item Therefore, total space is $O(f(n) \cdot \log t) = O(f(n)^2)$.
\end{itemize}

Thus, a deterministic Turing machine can simulate the NTM using $O(f(n)^2)$ space.
\end{proof}

\begin{remark}
The key insight is that while time is inherently sequential and expensive to simulate deterministically, space can be reused. Savitch's algorithm uses divide-and-conquer to recursively check for reachability in the configuration graph of the machine.
\end{remark}


\section{The Classes \textsf{L} and \textsf{NL}}

\subsection{Definitions}

\begin{definition}[Logarithmic Space Complexity]
Let $M$ be a Turing machine with:
\begin{itemize}
    \item A read-only input tape
    \item A read/write work tape
    \item (For nondeterministic machines) A read-once witness tape
\end{itemize}
We say $M$ operates in \emph{logarithmic space} if it uses at most $O(\log n)$ cells on its work tape(s) for inputs of size $n$.
\end{definition}

\begin{definition}[The Class \textsf{L}]
\textsf{L} is the class of languages decidable by deterministic Turing machines in logarithmic space.
\end{definition}

\begin{definition}[The Class \textsf{NL}]
\textsf{NL} is the class of languages decidable by nondeterministic Turing machines in logarithmic space.
\end{definition}

\subsection{Open Problems}

\begin{itemize}
    \item Does \textsf{L} = \textsf{NL}? (Widely believed to be false)
    \item Is \textsf{NL} = \textsf{P}? (Related to whether we can derandomize space-bounded computation)
\end{itemize}

\section{Space Complexity of LADDER Problem}

\subsection{Problem Definition}
The \boldmath$LADDER_{DFA}$ problem consists of:
\begin{itemize}
    \item A deterministic finite automaton (DFA) $D = (Q, \Sigma, \delta, q_0, F)$
    \item Two strings $s$ and $t$ of equal length $n$ over $\Sigma$
\end{itemize}
We must decide if there exists a sequence of strings from $s$ to $t$ where:
\begin{enumerate}
    \item Each intermediate string is accepted by $D$
    \item Consecutive strings differ in exactly one character position
\end{enumerate}

\subsection*{Recursive Algorithm for \boldmath$LADDER_{DFA}$}

Let $\texttt{isReachable}(x, t, d)$ be a recursive function that returns \texttt{true} if string $t$ is reachable from string $x$ using a ladder of at most $d$ steps, such that all intermediate strings are in $L(D)$ and each step changes exactly one character.

\begin{algorithm}[H]
\caption{Recursive Reachability Check}
\begin{algorithmic}[1]
\Function{isReachable}{$x, t, d$}
    \If{$x = t$}
        \State \Return true
    \EndIf
    \If{$d = 0$}
        \State \Return false
    \EndIf
    \For{each string $y$ such that $y$ differs from $x$ in exactly one position}
        \If{$y \in L(D)$}
            \If{\Call{isReachable}{$y, t, d-1$}}
                \State \Return true
            \EndIf
        \EndIf
    \EndFor
    \State \Return false
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Main idea:} Call \texttt{isReachable}($s, t, |\Sigma|^n$), the maximum number of ladder steps possible. In practice, a tighter bound suffices (e.g., $n \cdot (|\Sigma| - 1)$), but exponential bounds still allow a PSPACE algorithm if space is carefully bounded.

\subsection*{Why the Algorithm is in PSPACE}

To prove that the recursive algorithm runs in PSPACE, observe the following:

\begin{itemize}
    \item The input strings $x$ and $t$ are each of length $n$.
    \item Each recursive call adds only a small amount of space: $O(n)$ to store the current string and loop variables.
    \item The maximum recursion depth is at most $|\Sigma|^n$, but **we do not store the whole path**, only one string per level.
    \item Each string’s membership in $L(D)$ can be checked in polynomial time using the DFA.
    \item The space used is proportional to the maximum depth of the call stack multiplied by the space used per call. However, since we recompute neighbors on the fly and don’t store the full graph or path, we reuse space.
\end{itemize}

Thus, we are performing a depth-first search over an implicitly defined exponential-size graph, but without storing the graph or the visited set explicitly.

\subsubsection*{Savitch’s Theorem Connection}

Savitch’s Theorem states that for any function $f(n) \ge \log n$,
\[
NSPACE(f(n)) \subseteq DSPACE(f(n)^2)
\]
Since reachability in this exponential graph can be done nondeterministically in $O(n)$ space (guess the next string, check $L(D)$ membership), it can be done deterministically in $O(n^2)$ space.

\subsection*{Conclusion}

Even though the number of possible paths is exponential, the algorithm checks them one at a time using depth-first recursion and polynomial space per call. Thus, the recursive algorithm for $LADDER_{DFA}$ runs in PSPACE.
