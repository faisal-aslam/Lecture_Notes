\documentclass[12pt,a4paper]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{array}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{float}  % Added for better float control
\usepackage{tabularx}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}

\usepackage{mathtools} % Bonus
\DeclarePairedDelimiter\norm\lVert\rVert

\lstset{
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{white},
    frame=single,
    rulecolor=\color{black},
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showstringspaces=false
}

\setstretch{1.2}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{1em}{}

\newtcolorbox{definitionbox}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=Definition,
    sharp corners
}

\newtcolorbox{examplebox}{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Example,
    sharp corners
}

\newtcolorbox{importantbox}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=Important,
    sharp corners
}

\begin{document}

\begin{center}
    {\LARGE \textbf{Data Compression}}\\[0.3em]
    {\Large \textbf{Lecture Notes}}\\[0.5em]
    \textbf{Dr. Faisal Aslam}\\[1em]
\end{center}


\section{Lecture 1: Introduction to Data Compression}
\subsection{Learning Objectives}
By the end of this lecture, students will be able to:
\begin{itemize}[leftmargin=*]
    \item Define data compression and explain its practical importance with real-world examples
    \item Differentiate between lossless and lossy compression with concrete applications
    \item Calculate and interpret basic compression metrics (compression ratio, bit-rate, savings)
    \item Explain the concepts of information, redundancy, and entropy with computational examples
    \item Identify major application domains and their specific compression requirements
    \item Understand the fundamental limits of compression from information theory
\end{itemize}

\subsection{Introduction and Motivation: Why Compress Data?}
Data compression is the process of encoding information using fewer bits than the original representation. Every day, we encounter compression without realizing it: from streaming videos to sending emails, from saving photos to downloading software updates.

\begin{definitionbox}
\textbf{Data Compression}: The process of reducing the number of bits needed to represent information, while either:
\begin{itemize}
    \item \textbf{Lossless}: Preserving all original information exactly
    \item \textbf{Lossy}: Accepting some controlled loss of information for higher compression
\end{itemize}
\end{definitionbox}

\subsubsection{Real-World Motivation: A Concrete Example}
Consider a typical smartphone photo: 12 megapixels, 24-bit color (8 bits per RGB channel). Uncompressed size:
\[
12,000,000 \text{ pixels} \times 24 \text{ bits/pixel} = 288,000,000 \text{ bits} = 36 \text{ MB}
\]
But your phone stores it as a ~3 MB JPEG file. That's a 12:1 compression ratio! Without compression:
\begin{itemize}
    \item Your 128 GB phone could store only ~3,500 photos instead of ~40,000
    \item Uploading to social media would take 12 times longer
    \item Cloud storage costs would be 12 times higher
\end{itemize}

\subsubsection{The Economics of Compression}
\begin{examplebox}
\textbf{Cloud Storage Example}: A major cloud provider charges \$0.023 per GB per month. For 1 PB (petabyte = 1000 TB) of data:
\begin{itemize}
    \item Uncompressed: 1 PB = 1,000,000 GB gives \$23,000/month
    \item With 4:1 compression: 250,000 GB gives \$5,750/month
    \item Annual savings: (\$23,000 - \$5,750) × 12 = \$207,000/year
\end{itemize}
This doesn't even consider bandwidth costs, which are typically charged per GB transferred!
\end{examplebox}

\subsection{Lossless vs. Lossy Compression: A Detailed Comparison}
\subsubsection{Lossless Compression: Perfect Reconstruction}
\textbf{How it works}: Exploits statistical redundancy and patterns without losing information.

\textbf{Key techniques}:
\begin{enumerate}
    \item \textbf{Entropy coding}: Assign shorter codes to frequent symbols (Huffman, Arithmetic)
    \item \textbf{Dictionary methods}: Replace repeated patterns with references (LZ77, LZ78)
    \item \textbf{Predictive coding}: Encode differences from predictions rather than raw values
\end{enumerate}

\begin{examplebox}
\textbf{Text Compression Example}: The word "compression" appears 100 times in a document.
\begin{itemize}
    \item Uncompressed: "compression" = 11 characters $\times$ 8 bits = 88 bits $\times$ 100 = 8,800 bits
    \item Compressed: Assign code "01" (2 bits) for "compression" $\rightarrow$ 2 bits $\times$ 100 = 200 bits
    \item Plus dictionary entry: "compression" = 88 bits (stored once)
    \item Total: 200 + 88 = 288 bits vs 8,800 bits $\rightarrow$ 30:1 compression!
\end{itemize}
This is essentially how LZW (used in GIF, ZIP) works.
\end{examplebox}

\subsubsection{Lossy Compression: Intelligent Approximation}
\textbf{How it works}: Removes information that is:
\begin{itemize}
    \item Imperceptible to humans (psychovisual/psychoacoustic models)
    \item Less important for the intended use
    \item Redundant beyond a certain quality threshold
\end{itemize}

\begin{examplebox}
\textbf{JPEG Image Compression - Step by Step}:
\begin{enumerate}
    \item \textbf{Color Space Conversion}: RGB to YCbCr (separates luminance from color)
    \item \textbf{Chrominance Downsampling}: Reduce color resolution (4:2:0) - humans are less sensitive to color details
    \item \textbf{Discrete Cosine Transform (DCT)}: Convert 8×8 pixel blocks to frequency domain
    \item \textbf{Quantization}: Divide frequency coefficients by quantization matrix - small high-frequency coefficients become zero
    \item \textbf{Entropy Coding}: Huffman code the results

    \textbf{Result}: Typical 10:1 to 20:1 compression with minimal visible artifacts
\end{enumerate}
\end{examplebox}

\subsubsection{When to Use Which? Decision Factors}
% Using tabularx for better table control
\begin{table}[htbp]
\centering
\begin{tabularx}{\textwidth}{|p{3.5cm}|X|X|}
\hline
\textbf{Factor} & \textbf{Choose Lossless When} & \textbf{Choose Lossy When} \\
\hline
\textbf{Fidelity Requirement} & Exact reconstruction is critical (code, financial data, legal documents) & Some quality loss is acceptable (media streaming, web images) \\
\hline
\textbf{Data Type} & Discrete data with exact values (text, databases, executables) & Continuous data with perceptual limits (images, audio, video) \\
\hline
\textbf{Compression Ratio Needed} & Moderate ratios suffice (2:1 to 10:1) & High ratios needed (10:1 to 200:1+) \\
\hline
\textbf{Processing Requirements} & Fast decompression needed, encode speed less critical & Real-time encoding/decoding needed (streaming, videoconferencing) \\
\hline
\textbf{Regulatory Constraints} & Legal/medical requirements mandate exact copies & No regulatory constraints on quality \\
\hline
\end{tabularx}
\caption{Decision Factors for Lossless vs. Lossy Compression}
\end{table}

\subsection{Performance Metrics: Beyond Simple Ratios}
\subsubsection{Compression Ratio and Savings}

\begin{align*}
\text{Compression Ratio} &= \frac{\text{Original Size}}{\text{Compressed Size}} \\
\text{Savings2} = \left(1 - \frac{\text{Compressed Size}}{\text{Original Size}}\right) \times 100\%
\end{align*}




\begin{examplebox}
\textbf{Comparing Different Compression Scenarios}:
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Scenario} & \textbf{Original} & \textbf{Compressed} & \textbf{Ratio} & \textbf{Savings} \\
\hline
Text document (ZIP) & 1.5 MB & 450 KB & 3.33:1 & 70\% \\
\hline
CD Audio (FLAC lossless) & 700 MB & 350 MB & 2:1 & 50\% \\
\hline
Same Audio (MP3 128kbps) & 700 MB & 112 MB & 6.25:1 & 84\% \\
\hline
4K Video (H.265) & 100 GB & 2 GB & 50:1 & 98\% \\
\hline
DNA sequence (specialized) & 3 GB & 300 MB & 10:1 & 90\% \\
\hline
\end{tabular}
\end{center}
\end{examplebox}

\subsubsection{Bit-rate: The Quality Control Knob}
For lossy compression, bit-rate determines quality:
\[
\text{Bit-rate} = \frac{\text{Compressed Size in bits}}{\text{Duration (seconds)}} \quad \text{or} \quad \frac{\text{Compressed Size in bits}}{\text{Number of samples}}
\]

\begin{examplebox}
\textbf{Audio Quality at Different Bit-rates}:
\begin{itemize}
    \item \textbf{32 kbps}: Telephone quality, speech only
    \item \textbf{96 kbps}: FM radio quality
    \item \textbf{128 kbps}: "Good enough" for most listeners
    \item \textbf{192 kbps}: Near CD quality for most people
    \item \textbf{320 kbps}: Essentially transparent (FLAC: ~900 kbps)

    \textbf{Storage impact}: A 60-minute album:
    \begin{itemize}
        \item At 128 kbps: 60 MB
        \item At 320 kbps: 144 MB
        \item FLAC lossless: ~400 MB
        \item Uncompressed CD: 700 MB
    \end{itemize}
\end{itemize}
\end{examplebox}

\subsubsection{Time and Space Trade-offs}
Compression involves multiple dimensions:
\[
\text{Space-Time Trade-off} = \frac{\text{Compression Ratio}}{\text{Encoding Time} \times \text{Decoding Time}}
\]

\begin{examplebox}
\textbf{Real-world Compressor Comparison}:
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Ratio (text)} & \textbf{Encode Speed} & \textbf{Decode Speed} & \textbf{Memory} \\
\hline
gzip (-6) & 3.2:1 & 100 MB/s & 400 MB/s & 10 MB \\
\hline
bzip2 (-6) & 3.8:1 & 20 MB/s & 50 MB/s & 50 MB \\
\hline
LZ4 & 2.5:1 & 500 MB/s & 2000 MB/s & 1 MB \\
\hline
Zstd (-3) & 3.0:1 & 300 MB/s & 800 MB/s & 5 MB \\
\hline
xz (-6) & 4.2:1 & 10 MB/s & 80 MB/s & 100 MB \\
\hline
\end{tabular}
\end{center}
Approximate performance on typical text data (higher is better)
\end{examplebox}

\subsection{Information and Redundancy: The Core Concepts}
\subsubsection{Information: Quantifying Surprise}
Claude Shannon's revolutionary insight: Information is inversely related to probability.

\begin{definitionbox}
\textbf{Information Content} of an event with probability $p$:
\[
I(p) = -\log_2 p \quad \text{bits}
\]
\end{definitionbox}

\begin{examplebox}
\textbf{Daily Weather Forecast - Information Content}:
\begin{itemize}
    \item Sunny in Phoenix (probability 0.9): $I = -\log_2 0.9 \approx 0.15$ bits
    \item Snow in Phoenix (probability 0.001): $I = -\log_2 0.001 \approx 9.97$ bits
    \item Rain in Seattle (probability 0.3): $I = -\log_2 0.3 \approx 1.74$ bits

    \textbf{Interpretation}: Rare events carry more information! Snow in Phoenix tells you much more about the weather pattern than yet another sunny day.
\end{itemize}
\end{examplebox}

\subsubsection{Redundancy: The Enemy of Compression}
Redundancy comes in several forms:

\begin{enumerate}
    \item \textbf{Spatial Redundancy}: Neighboring pixels are correlated
    \begin{examplebox}
    In a blue sky photo, most pixels are similar shades of blue. Instead of storing each pixel independently:
    \begin{itemize}
        \item Naive: RGB values for each of 1 million pixels
        \item Smart: "The next 1000 pixels are color (135, 206, 235)" - Run-length encoding
        \item Even smarter: Predict each pixel from its neighbors, encode only differences
    \end{itemize}
    \end{examplebox}

    \item \textbf{Statistical Redundancy}: Uneven symbol frequencies
    \begin{examplebox}
    English text letter frequencies:
    \begin{center}
    \begin{tabular}{|c|c||c|c|}
    \hline
    Letter & Frequency & Letter & Frequency \\
    \hline
    E & 12.7\% & Z & 0.07\% \\
    T & 9.1\% & Q & 0.10\% \\
    A & 8.2\% & J & 0.15\% \\
    \hline
    \end{tabular}
    \end{center}
    \textbf{Inefficient}: Fixed 5 bits per letter (32 possible)
    \textbf{Efficient}: Huffman coding: E = 3 bits, Z = 9 bits
    Average bits per letter drops from 5 to ~4.1
    \end{examplebox}

    \item \textbf{Knowledge Redundancy}: Information known to both encoder and decoder
    \begin{examplebox}
    \textbf{Medical Imaging}: Both sides know the image represents a chest X-ray:
    \begin{itemize}
        \item Don't need to encode that lungs should be in certain positions
        \item Can use anatomical models to predict and encode differences
        \item Can focus bits on diagnostically important regions
    \end{itemize}
    \end{examplebox}

    \item \textbf{Perceptual Redundancy}: Information humans can't perceive
    \begin{examplebox}
    \textbf{Audio Compression (MP3)}:
    \begin{itemize}
        \item \textbf{Frequency masking}: A loud sound at 1 kHz makes nearby frequencies (950-1050 Hz) inaudible
        \item \textbf{Temporal masking}: A loud sound makes preceding/following quiet sounds inaudible
        \item \textbf{Result}: Can discard ~90\% of audio data without audible difference
    \end{itemize}
    \end{examplebox}
\end{enumerate}

\subsection{Entropy: The Fundamental Limit}
\subsubsection{Calculating Entropy: Step by Step}
Entropy is the average information content per symbol:

\begin{definitionbox}
\textbf{Entropy} of a discrete source with symbols $s_1, s_2, \ldots, s_n$ having probabilities $p_1, p_2, \ldots, p_n$:
\[
H = -\sum_{i=1}^{n} p_i \log_2 p_i \quad \text{bits per symbol}
\]
\end{definitionbox}

\begin{examplebox}
\textbf{Binary Source Example - Detailed Calculation}:
Consider a biased coin: P(Heads) = 0.8, P(Tails) = 0.2

\begin{enumerate}
    \item Information content of Heads: $I_H = -\log_2 0.8 \approx 0.3219$ bits
    \item Information content of Tails: $I_T = -\log_2 0.2 \approx 2.3219$ bits
    \item Entropy: $H = 0.8 \times 0.3219 + 0.2 \times 2.3219 = 0.7219$ bits
\end{enumerate}

\textbf{Interpretation}:
\begin{itemize}
    \item On average, each coin flip gives us 0.72 bits of information
    \item We need at least 0.72 bits per flip to encode the sequence
    \item If coins were fair (P=0.5), $H = 1.0$ bit - maximum uncertainty
    \item If always heads (P=1.0), $H = 0$ bits - no information
\end{itemize}
\end{examplebox}

\subsubsection{Entropy of English Text}
\begin{examplebox}
\textbf{Calculating English Letter Entropy}:
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Letter & Probability & $-\log_2 p$ & Contribution \\
\hline
E & 0.127 & 2.98 & 0.378 \\
T & 0.091 & 3.46 & 0.315 \\
A & 0.082 & 3.61 & 0.296 \\
... & ... & ... & ... \\
Z & 0.0007 & 10.48 & 0.007 \\
\hline
\textbf{Total} & & & \textbf{4.18 bits} \\
\hline
\end{tabular}
\end{center}

\textbf{What this means}:
\begin{itemize}
    \item \textbf{Naive encoding}: 5 bits per letter (32 possibilities)
    \item \textbf{Entropy limit}: 4.18 bits per letter
    \item \textbf{Practical Huffman}: ~4.3 bits per letter
    \item \textbf{With word models}: ~2.3 bits per letter (exploiting word-level patterns)
    \item \textbf{With context}: ~1.5 bits per letter (exploiting grammar, semantics)
\end{itemize}
\end{examplebox}

\subsubsection{The Entropy Theorem: Why It Matters}
\begin{importantbox}
\textbf{Shannon's Source Coding Theorem (Informal)}:
\begin{itemize}
    \item \textbf{Lower bound}: No lossless compressor can average fewer than $H$ bits/symbol
    \item \textbf{Upper bound}: You can get arbitrarily close to $H$ bits/symbol
    \item \textbf{Implication}: Entropy is the absolute limit for lossless compression

    \textbf{Example}: For English letters ($H = 4.18$ bits):
    \begin{itemize}
        \item Impossible: Average $<$ 4.18 bits/letter
        \item Possible but wasteful: 8 bits/letter (ASCII)
        \item Good: 4.3 bits/letter (Huffman)
        \item Approaching limit: 4.19 bits/letter (Arithmetic with context)
    \end{itemize}
\end{itemize}
\end{importantbox}

\subsection{Application Domains: Specialized Requirements}
\subsubsection{Text and Code Compression}
\begin{itemize}
    \item \textbf{Requirements}: Lossless, fast random access, incremental updates
    \item \textbf{Challenges}: Small files, need for searching within compressed data
    \item \textbf{Solutions}: gzip (DEFLATE), LZ4, Zstandard
   \begin{examplebox}
\textbf{Git Version Control}: Uses zlib (DEFLATE) and delta compression:
\begin{itemize}
    \item Stores file versions as compressed objects
    \item Applies delta compression for similar versions (packfiles)
    \item Exploits low \emph{conditional entropy} between revisions
    \item Example: Linux kernel repository: $\sim$4\,GB raw, $\sim$1\,GB stored
\end{itemize}
\end{examplebox}

\end{itemize}

\subsubsection{Multimedia Compression}
\begin{itemize}
    \item \textbf{Requirements}: High compression, perceptual quality, real-time
    \item \textbf{Challenges}: Massive data volumes, human perception constraints
    \item \textbf{Solutions}: JPEG, MP3, H.264/HEVC, AV1
    \begin{examplebox}
    \textbf{Streaming Service Economics (Netflix/YouTube)}:
    \begin{itemize}
        \item 1 hour of 4K video: Uncompressed ~500 GB
        \item H.265 compressed: ~4 GB (125:1 compression)
        \item Bandwidth cost: \$0.05/GB (typical CDN pricing)
        \item Uncompressed stream: \$25/hour
        \item Compressed stream: \$0.20/hour
        \item For 100 million hours/day: \$20M/day vs \$2.5B/day!
    \end{itemize}
    \end{examplebox}
\end{itemize}

\subsubsection{Scientific and Medical Data}
\begin{itemize}
    \item \textbf{Requirements}: Lossless or controlled loss, reproducibility, standards
    \item \textbf{Challenges}: Huge datasets, precision requirements, regulatory compliance
    \item \textbf{Solutions}: Specialized compressors (SZ, ZFP), format standards (DICOM)
    \begin{examplebox}
    \textbf{Large Hadron Collider (LHC) Data}:
    \begin{itemize}
        \item Generates 1 PB/second (yes, per second!)
        \item Stores 50 PB/year after filtering
        \item Uses specialized compression algorithms
        \item Compression saves ~\$50M/year in storage costs
        \item Enables global collaboration (data distributed worldwide)
    \end{itemize}
    \end{examplebox}
\end{itemize}

\subsection{The Compression Pipeline: How Compressors Actually Work}
Most compressors follow this two-stage process:

% Simple text-based diagram
\begin{center}
\textbf{Compression Pipeline:}

\medskip
\begin{tabular}{cccc}
\textbf{Input Data} & $\longrightarrow$ & \textbf{Modeling Stage} & $\longrightarrow$ \\
& \scriptsize{Analyzes patterns} & & \scriptsize{Builds probability model} \\
\end{tabular}

\medskip
\begin{tabular}{cccc}
& $\longrightarrow$ & \textbf{Coding Stage} & $\longrightarrow$ \\
& & \scriptsize{Converts to bits} & \scriptsize{Using entropy coding} \\
\end{tabular}

\medskip
\begin{tabular}{cccc}
& $\longrightarrow$ & \textbf{Compressed Data} \\
\end{tabular}
\end{center}


\textbf{Two-Stage Compression Pipeline}:
\begin{itemize}
    \item \textbf{Modeling Stage}: Analyzes data patterns and builds probability model
    \item \textbf{Coding Stage}: Converts symbols to bits using entropy coding (Huffman, Arithmetic, ANS)
\end{itemize}

\subsubsection{Modeling Strategies in Practice}
\begin{examplebox}
\textbf{Huffman Coding Example - Complete Process}:
\begin{enumerate}
    \item \textbf{Modeling}: Count symbol frequencies in "ABRACADABRA"
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Symbol & Frequency & Probability \\
    \hline
    A & 5 & 5/11 $\approx$ 0.455 \\
    B & 2 & 2/11 $\approx$ 0.182 \\
    R & 2 & 2/11 $\approx$ 0.182 \\
    C & 1 & 1/11 $\approx$ 0.091 \\
    D & 1 & 1/11 $\approx$ 0.091 \\
    \hline
    \end{tabular}
    \end{center}

    \item \textbf{Coding}: Build Huffman tree (simplified):
    \begin{itemize}
        \item Combine lowest frequencies: C(1) + D(1) = CD(2)
        \item Continue combining: CD(2) + B(2) = CDB(4)
        \item Combine: CDB(4) + R(2) = CDBR(6)
        \item Final: CDBR(6) + A(5) = Root(11)
    \end{itemize}

    \item \textbf{Code assignment}:
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Symbol & Code & Length \\
    \hline
    A & 0 & 1 bit \\
    R & 10 & 2 bits \\
    B & 110 & 3 bits \\
    C & 1110 & 4 bits \\
    D & 1111 & 4 bits \\
    \hline
    \end{tabular}
    \end{center}

    \item \textbf{Compress "ABRACADABRA"}:
    \begin{itemize}
        \item A(0) B(110) R(10) A(0) C(1110) A(0) D(1111) A(0) B(110) R(10) A(0)
        \item Total bits: 1+3+2+1+4+1+4+1+3+2+1 = 23 bits
        \item Original: 11 characters $\times$ 8 bits = 88 bits
        \item Compression: 88 $\rightarrow$ 23 bits (3.8:1 ratio)
        \item Entropy limit: $H \approx 2.04$ bits/char $\times$ 11 = 22.5 bits
        \item Efficiency: 22.5/23 = 97.8\% efficient!
    \end{itemize}
\end{enumerate}
\end{examplebox}

\subsection{Important Terminology and Concepts}
\subsubsection{Key Definitions with Examples}
\begin{itemize}
    \item \textbf{Symbol}: The basic unit being compressed
    \begin{examplebox}
    Different domains use different symbols:
    \begin{itemize}
        \item Text: Characters (bytes)
        \item Images: Pixels (RGB triples)
        \item Audio: Samples (16-bit integers)
        \item Video: Macroblocks (16$\times$16 pixel regions)
    \end{itemize}
    \end{examplebox}

    \item \textbf{Alphabet}: Set of all possible symbols
    \begin{examplebox}
    \begin{itemize}
        \item English text: 256 possible bytes (ASCII/UTF-8)
        \item Binary data: 256 possible byte values
        \item DNA sequences: 4 symbols \{A, C, G, T\}
        \item Black-white image: 2 symbols \{0=black, 1=white\}
    \end{itemize}
    \end{examplebox}

    \item \textbf{Prefix Code}: Crucial for instant decoding
    \begin{examplebox}
    \textbf{Why prefix codes matter}:
    \begin{itemize}
        \item Good: A=0, B=10, C=110, D=111
        \item "010110" decodes unambiguously: A(0) B(10) C(110)
        \item Bad: A=0, B=1, C=01 (not prefix-free)
        \item "01" could be AB or C - ambiguous!
    \end{itemize}
    \end{examplebox}
\end{itemize}

\subsubsection{The Fundamental Insight}
\begin{importantbox}
\textbf{The Core Principle of Compression}:
\begin{itemize}
    \item \textbf{Random data cannot be compressed}: Maximum entropy = no redundancy
    \item \textbf{Real-world data is not random}: Contains patterns, structure, predictability
    \item \textbf{Compression finds and exploits these patterns}

    \textbf{Example - Encryption vs Compression}:
    \begin{itemize}
        \item Encrypted data looks random (high entropy)
        \item Compressing encrypted data gives little or no savings
        \item Always compress \textbf{before} encrypting, not after!
        \item Rule: Encrypt $\rightarrow$ High entropy $\rightarrow$ No compression
        \item Rule: Compress $\rightarrow$ Lower entropy $\rightarrow$ Then encrypt
    \end{itemize}
\end{itemize}
\end{importantbox}

\subsection{Homework Assignment: Practical Exercises}
\begin{enumerate}
    \item \textbf{Compression Calculation}:
    \begin{itemize}
        \item A 4K video frame is 3840$\times$2160 pixels, 24-bit color. Calculate:
        \begin{enumerate}
            \item Uncompressed size in MB
            \item Size after 10:1 compression
            \item Size after 50:1 compression
            \item For a 2-hour movie at 24 fps, calculate total sizes
        \end{enumerate}
    \end{itemize}

    \item \textbf{Entropy Calculation}:
    \begin{itemize}
        \item Calculate entropy for these sources:
        \begin{enumerate}
            \item A die roll (6 equally likely outcomes)
            \item Weather: Sunny(0.6), Cloudy(0.3), Rainy(0.1)
            \item Binary source: P(0)=0.99, P(1)=0.01
        \end{enumerate}
        \item Which is most compressible? Why?
    \end{itemize}

    \item \textbf{Real-world Analysis}:
    \begin{itemize}
        \item Take three files from your computer: a .txt document, a .jpg image, and a .zip file
        \item Record their sizes
        \item Compress them using gzip at maximum compression
        \item Calculate compression ratios
        \item Explain why they compress differently
    \end{itemize}

    \item \textbf{Huffman Coding Practice}:
    \begin{itemize}
        \item For the message "MISSISSIPPI":
        \begin{enumerate}
            \item Calculate symbol frequencies
            \item Build Huffman tree
            \item Assign codes
            \item Encode the message
            \item Calculate compression ratio vs 8-bit ASCII
            \item Compare to entropy limit
        \end{enumerate}
    \end{itemize}

    \item \textbf{Research and Analysis}:
    \begin{itemize}
        \item Find a current research paper on neural compression
        \item Summarize its approach in 200 words
        \item Compare its claimed performance to traditional methods
        \item Identify one advantage and one limitation
    \end{itemize}
\end{enumerate}

\subsection{Looking Ahead: What's Next?}
In the next lecture, we will dive deeper into:
\begin{itemize}
    \item \textbf{Shannon's Source Coding Theorem}: Formal statement and proof
    \item \textbf{Kraft-McMillan Inequality}: Mathematical foundation of prefix codes
    \item \textbf{Optimal Code Construction}: How to achieve the entropy limit
    \item \textbf{Practical Implications}: What these theorems mean for real compressors
\end{itemize}

\begin{importantbox}
\textbf{Key Takeaways from Lecture 1}:
\begin{enumerate}
    \item Compression is economically and practically essential in modern computing
    \item Lossless vs lossy involves trade-offs between fidelity and compression ratio
    \item Entropy defines the absolute limit for lossless compression
    \item Real compressors work by modeling data patterns, then encoding efficiently
    \item Different applications require specialized compression approaches
\end{enumerate}
\end{importantbox}


%---------------------------- End of lecture 1


\section{Lecture 2: Shannon's Source Coding Theorem and Kraft-McMillan Inequality}
\subsection{Learning Objectives}
By the end of this lecture, students will be able to:
\begin{itemize}[leftmargin=*]
    \item Formally state and prove Shannon's Source Coding Theorem for discrete memoryless sources
    \item Apply the Kraft-McMillan inequality to characterize uniquely decodable codes
    \item Construct optimal prefix codes and analyze their properties
    \item Derive and interpret the relationship between entropy and achievable compression rates
    \item Compute code efficiency, redundancy, and performance bounds
    \item Understand the mathematical foundations of lossless compression limits
\end{itemize}

\subsection{Mathematical Preliminaries and Notation}
Let $X$ be a discrete random variable taking values in alphabet $\mathcal{X} = \{x_1, x_2, \ldots, x_m\}$ with probability mass function $p(x) = \Pr(X = x)$.

\begin{definitionbox}
\textbf{Source Code}: A mapping $C: \mathcal{X} \to \mathcal{D}^*$ where $\mathcal{D} = \{0, 1\}$ is the code alphabet, and $\mathcal{D}^*$ is the set of all finite binary strings. The code $C$ assigns to each symbol $x_i$ a codeword $c_i$ of length $\ell_i = |c_i|$.
\end{definitionbox}

\subsubsection{Expected Code Length}
For a source with probabilities $p_1, p_2, \ldots, p_m$ and corresponding codeword lengths $\ell_1, \ell_2, \ldots, \ell_m$, the expected code length is:
\[
L(C) = \mathbb{E}[\ell(X)] = \sum_{i=1}^m p_i \ell_i
\]

\subsection{Shannon's Source Coding Theorem: Formal Statement}
\begin{definitionbox}
\textbf{Shannon's Source Coding Theorem (1948)}: For any discrete memoryless source $X$ with entropy $H(X)$ and any uniquely decodable code $C$, the expected length $L(C)$ satisfies:
\[
H(X) \leq L(C) < H(X) + 1
\]
Moreover, for the $n$th extension of the source (coding $n$ symbols together), there exists a uniquely decodable code $C_n$ such that:
\[
\frac{1}{n} L(C_n) \to H(X) \quad \text{as } n \to \infty
\]
\end{definitionbox}

\subsubsection{Interpretation and Significance}
\begin{itemize}
    \item \textbf{Fundamental Limit}: $H(X)$ bits/symbol is the absolute minimum for lossless compression
    \item \textbf{Achievability}: We can get arbitrarily close to this limit by coding in blocks
    \item \textbf{Penalty Term}: The "+1" represents overhead from integer codeword lengths
\end{itemize}

\begin{examplebox}
\textbf{Binary Source Analysis}: Consider a binary source with $P(0) = p$, $P(1) = 1-p$:
\begin{itemize}
    \item Entropy: $H(p) = -p\log_2 p - (1-p)\log_2(1-p)$
    \item For $p = 0.1$: $H(0.1) \approx 0.469$ bits/symbol
    \item Theorem guarantees: $0.469 \leq L < 1.469$ bits/symbol
    \item Simple code: 0→0, 1→1 gives $L = 1$ bit/symbol (efficiency 46.9\%)
    \item Block coding can approach 0.469 bits/symbol
\end{itemize}
\end{examplebox}

\subsection{Code Classification and Properties}
\subsubsection{Hierarchical Classification of Codes}
\begin{center}
\textbf{Hierarchy of Codes:}

\medskip
\begin{tabular}{c}
All Codes \\
$\downarrow$ \\
Non-singular Codes \\
$\downarrow$ \\
Uniquely Decodable Codes \\
$\downarrow$ \\
Prefix Codes
\end{tabular}
\end{center}


\subsubsection{Formal Definitions}
\begin{enumerate}
    \item \textbf{Non-singular}: $C(x_i) \neq C(x_j)$ for $i \neq j$
    \item \textbf{Uniquely Decodable}: Extension $C^n$ is non-singular for all $n$
    \item \textbf{Prefix (Instantaneous)}: No codeword is a prefix of another
\end{enumerate}

\begin{examplebox}
\textbf{Code Classification Examples}:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Code} & \textbf{Mapping} & \textbf{Singular?} & \textbf{Uniquely Decodable?} & \textbf{Prefix?} \\
\hline
$C_1$ & a→0, b→0, c→1 & Yes & No & No \\
\hline
$C_2$ & a→0, b→01, c→11 & No & No & No \\
\hline
$C_3$ & a→0, b→01, c→011 & No & Yes & No \\
\hline
$C_4$ & a→0, b→10, c→110 & No & Yes & Yes \\
\hline
\end{tabular}
\caption{Classification of different codes for alphabet $\{a,b,c\}$}
\end{table}

\textbf{Analysis of $C_3$}: Code "011" could be decoded as "ab" or "c" - ambiguous!
\end{examplebox}

\subsection{Kraft-McMillan Inequality: Mathematical Foundation}
\begin{theorem}[Kraft-McMillan Inequality]
For any prefix code (or more generally, any uniquely decodable code) with codeword lengths $\ell_1, \ell_2, \ldots, \ell_m$ over a $D$-ary alphabet:
\[
\sum_{i=1}^m D^{-\ell_i} \leq 1
\]
where $D$ is the size of the code alphabet (2 for binary).
\end{theorem}

\subsubsection{Proof Sketch for Binary Prefix Codes}
\begin{enumerate}
    \item Consider a complete binary tree of depth $L = \max_i \ell_i$
    \item Each codeword of length $\ell_i$ occupies $2^{L-\ell_i}$ leaf positions
    \item Total occupied positions: $\sum_{i=1}^m 2^{L-\ell_i} \leq 2^L$
    \item Dividing by $2^L$: $\sum_{i=1}^m 2^{-\ell_i} \leq 1$
\end{enumerate}

\subsubsection{Converse: Constructing Codes from Lengths}
\begin{theorem}[Kraft Inequality Converse]
If integers $\ell_1, \ell_2, \ldots, \ell_m$ satisfy $\sum_{i=1}^m 2^{-\ell_i} \leq 1$, then there exists a binary prefix code with these lengths.
\end{theorem}

\begin{examplebox}
\textbf{Verifying Kraft Inequality}:
\begin{enumerate}
    \item Consider lengths $\{1, 2, 3, 3\}$:
    \[
    \sum 2^{-\ell_i} = 2^{-1} + 2^{-2} + 2^{-3} + 2^{-3} = 0.5 + 0.25 + 0.125 + 0.125 = 1
    \]
    A prefix code exists (e.g., 0, 10, 110, 111)

    \item Consider lengths $\{1, 1, 2\}$:
    \[
    \sum 2^{-\ell_i} = 2^{-1} + 2^{-1} + 2^{-2} = 0.5 + 0.5 + 0.25 = 1.25 > 1
    \]
    No prefix code exists with these lengths!
\end{enumerate}
\end{examplebox}

\subsection{Optimal Code Lengths and Shannon Coding}
\subsubsection{Shannon's Length Assignment}
For a source with probabilities $p_i$, Shannon proposed the length assignment:
\[
\ell_i = \lceil -\log_2 p_i \rceil
\]
where $\lceil x \rceil$ is the ceiling function.

\begin{theorem}
The lengths $\ell_i = \lceil -\log_2 p_i \rceil$ satisfy the Kraft inequality.
\end{theorem}

\begin{proof}
Since $\ell_i \geq -\log_2 p_i$, we have $-\ell_i \leq \log_2 p_i$, so:
\[
2^{-\ell_i} \leq p_i \quad \Rightarrow \quad \sum_{i=1}^m 2^{-\ell_i} \leq \sum_{i=1}^m p_i = 1
\]
\end{proof}

\begin{examplebox}
\textbf{Shannon Coding Example}: Source with probabilities $\{0.4, 0.3, 0.2, 0.1\}$
\begin{enumerate}
    \item Compute ideal lengths: $-\log_2 p_i = \{1.32, 1.74, 2.32, 3.32\}$
    \item Ceiling gives: $\ell_i = \{2, 2, 3, 4\}$
    \item Check Kraft: $2^{-2} + 2^{-2} + 2^{-3} + 2^{-4} = 0.25 + 0.25 + 0.125 + 0.0625 = 0.6875 \leq 1$
    \item Expected length: $L = 0.4\times2 + 0.3\times2 + 0.2\times3 + 0.1\times4 = 2.4$ bits/symbol
    \item Entropy: $H = 1.846$ bits/symbol
    \item Efficiency: $\eta = 1.846/2.4 = 76.9\%$
\end{enumerate}
\end{examplebox}

\subsection{Detailed Proof of Shannon's Theorem}
\subsubsection{Lower Bound: $L \geq H(X)$}
\begin{proof}
Let $p_i$ be symbol probabilities and $\ell_i$ be codeword lengths of a uniquely decodable code. From Kraft-McMillan:
\[
\sum_{i=1}^m 2^{-\ell_i} \leq 1
\]
Define $r_i = 2^{-\ell_i} / \sum_{j=1}^m 2^{-\ell_j}$, so $\{r_i\}$ is a probability distribution.

Using the non-negativity of KL-divergence:
\[
D(p \| r) = \sum_{i=1}^m p_i \log_2 \frac{p_i}{r_i} \geq 0
\]
Substituting $r_i$:
\[
\sum_{i=1}^m p_i \log_2 p_i - \sum_{i=1}^m p_i \log_2 2^{-\ell_i} + \sum_{i=1}^m p_i \log_2 \left(\sum_{j=1}^m 2^{-\ell_j}\right) \geq 0
\]
Since $\sum_{j=1}^m 2^{-\ell_j} \leq 1$, the last term is $\leq 0$, giving:
\[
-H(X) + \sum_{i=1}^m p_i \ell_i \geq 0 \quad \Rightarrow \quad L \geq H(X)
\]
\end{proof}

\subsubsection{Upper Bound: $L < H(X) + 1$}
\begin{proof}
Choose $\ell_i = \lceil -\log_2 p_i \rceil$. Then:
\[
-\log_2 p_i \leq \ell_i < -\log_2 p_i + 1
\]
Multiply by $p_i$ and sum over $i$:
\[
-\sum_{i=1}^m p_i \log_2 p_i \leq \sum_{i=1}^m p_i \ell_i < -\sum_{i=1}^m p_i \log_2 p_i + \sum_{i=1}^m p_i
\]
\[
H(X) \leq L < H(X) + 1
\]
\end{proof}

\subsection{Extended Source Coding and Block Codes}
\subsubsection{The $n$th Extension of a Source}
For a discrete memoryless source $X$, the $n$th extension $X^n = (X_1, X_2, \ldots, X_n)$ has:
\[
H(X^n) = nH(X)
\]
Applying Shannon's theorem to $X^n$ gives a code $C_n$ with:
\[
nH(X) \leq L(C_n) < nH(X) + 1
\]
Thus, the average length per symbol satisfies:
\[
H(X) \leq \frac{L(C_n)}{n} < H(X) + \frac{1}{n}
\]

\begin{examplebox}
\textbf{Block Coding Improvement}: Binary source with $p = 0.1$, $H = 0.469$ bits/symbol
\begin{itemize}
    \item Single symbol coding: Best code gives $L = 1$ bit/symbol (efficiency 46.9\%)
    \item Block coding with $n=2$: There are 4 possible blocks:
    \begin{align*}
        &00: p^2 = 0.81 \quad \ell = 1 \\
        &01: p(1-p) = 0.09 \quad \ell = 4 \\
        &10: p(1-p) = 0.09 \quad \ell = 4 \\
        &11: (1-p)^2 = 0.01 \quad \ell = 7
    \end{align*}
    \item Expected length: $L_2 = 0.81\times1 + 0.18\times4 + 0.01\times7 = 1.6$ bits/block
    \item Per symbol: $L_2/2 = 0.8$ bits/symbol (efficiency 58.6\%)
    \item For $n=10$: Efficiency approaches 90\%
\end{itemize}
\end{examplebox}

\subsection{Code Efficiency and Redundancy Analysis}
\subsubsection{Performance Metrics}
\begin{definitionbox}
For a code $C$ with expected length $L$ coding a source with entropy $H$:
\[
\text{Efficiency: } \eta = \frac{H}{L} \times 100\% \quad \text{Redundancy: } \rho = L - H
\]
\end{definitionbox}

\subsubsection{Theoretical Bounds}
From Shannon's theorem:
\[
\frac{H}{H+1} \leq \eta \leq 1 \quad \text{and} \quad 0 \leq \rho < 1
\]

\begin{examplebox}
\textbf{Efficiency vs. Entropy}:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$H$ (bits/symbol) & Minimum $\eta$ & Maximum $\rho$ & Interpretation \\
\hline
0.1 & 9.1\% & 0.9 bits & Very compressible, but +1 term dominates \\
\hline
1.0 & 50\% & 1.0 bit & Fair coin, maximum +1 overhead \\
\hline
2.0 & 66.7\% & 1.0 bit & +1 becomes less significant \\
\hline
4.0 & 80\% & 1.0 bit & High entropy, good efficiency possible \\
\hline
7.0 & 87.5\% & 1.0 bit & +1 overhead relatively small \\
\hline
\end{tabular}
\caption{Theoretical limits on code efficiency for different entropy values}
\end{table}
\end{examplebox}

\subsection{Algorithmic Construction of Prefix Codes}
\begin{algorithm}[H]
\caption{Canonical Prefix Code Construction from Lengths}
\begin{algorithmic}[1]
\REQUIRE Integer lengths $\ell_1 \leq \ell_2 \leq \cdots \leq \ell_m$ satisfying Kraft inequality
\ENSURE Binary prefix code with given lengths in canonical form
\STATE Initialize $code \gets 0$ (binary)
\FOR{$i = 1$ to $m$}
    \STATE Assign $c_i \gets$ first $\ell_i$ bits of $code$
    \STATE \textbf{Print} Symbol $i$: $c_i$ (length $\ell_i$)
    \STATE Increment $code$ by 1 (binary addition)
    \IF{$i < m$}
        \STATE Shift $code$ left by $\ell_{i+1} - \ell_i$ bits
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{examplebox}
\textbf{Canonical Code Construction}: Lengths $\{2, 2, 3, 3, 3\}$
\begin{enumerate}
    \item Start: $code = 00$
    \item $\ell_1=2$: $c_1 = 00$, increment → 01, no shift (same length)
    \item $\ell_2=2$: $c_2 = 01$, increment → 10, shift left 1 → 100
    \item $\ell_3=3$: $c_3 = 100$, increment → 101, no shift
    \item $\ell_4=3$: $c_4 = 101$, increment → 110, no shift
    \item $\ell_5=3$: $c_5 = 110$
\end{enumerate}
Result: $\{00, 01, 100, 101, 110\}$
\end{examplebox}

\subsection{Practical Implications and Limitations}
\subsubsection{Assumptions of Shannon's Theorem}
\begin{itemize}
    \item \textbf{Discrete Memoryless Source}: Symbols independent and identically distributed
    \item \textbf{Known Distribution}: Probabilities $p_i$ are known in advance
    \item \textbf{Arbitrary Delay}: Block coding allows infinite delay for encoding/decoding
    \item \textbf{No Complexity Constraints}: No limits on computational resources
\end{itemize}

\subsubsection{Violations in Practice}
\begin{examplebox}
\textbf{Real-world Violations}:
\begin{itemize}
    \item \textbf{Dependencies}: English text has strong correlations between letters
    \item \textbf{Unknown Distribution}: Must estimate probabilities from data
    \item \textbf{Delay Constraints}: Real-time applications limit block size
    \item \textbf{Complexity}: Exponential growth with block size ($m^n$ sequences)
\end{itemize}
\end{examplebox}

\subsection{Extensions and Generalizations}
\subsubsection{Markov Sources}
For a $k$th order Markov source with conditional entropy $H(X|X^{k})$, the theorem extends to:
\[
H(X|X^{k}) \leq L < H(X|X^{k}) + 1
\]

\subsubsection{Universal Coding}
When the source distribution is unknown, universal codes achieve:
\[
\frac{1}{n} L_n \to H(X) \quad \text{almost surely}
\]
Examples: Lempel-Ziv codes, arithmetic coding with adaptive models.

\subsubsection{Rate-Distortion Theory}
For lossy compression with distortion $D$, the rate-distortion function $R(D)$ gives the minimum achievable rate:
\[
R(D) = \min_{p(\hat{x}|x): \mathbb{E}[d(X,\hat{X})] \leq D} I(X;\hat{X})
\]

\subsection{Advanced Examples and Applications}
\begin{examplebox}
\textbf{DNA Sequence Compression}: Alphabet $\{A,C,G,T\}$ with typical probabilities $\{0.3, 0.2, 0.2, 0.3\}$
\begin{itemize}
    \item Entropy: $H = 1.97$ bits/base
    \item Simple code: 2 bits/base (efficiency 98.5\%)
    \item Exploiting dependencies: Adjacent bases are correlated in genomes
    \item Conditional entropy: $H(X_n|X_{n-1}) \approx 1.5$ bits/base
    \item Practical compressors achieve ~1.6 bits/base
\end{itemize}
\end{examplebox}

\begin{examplebox}
\textbf{Image Compression Limit}: Grayscale image with 256 levels
\begin{itemize}
    \item Naive: 8 bits/pixel
    \item Actual entropy from pixel correlations: Typically 1-4 bits/pixel
    \item PNG (lossless): 2-6 bits/pixel
    \item JPEG (lossy): 0.5-2 bits/pixel with visual quality
    \item Theoretical limit from image statistics
\end{itemize}
\end{examplebox}

\subsection{Homework Assignment: Advanced Problems}
\begin{enumerate}
    \item \textbf{Mathematical Proofs}:
    \begin{enumerate}
        \item Prove that for any uniquely decodable code, $\sum 2^{-\ell_i} \leq 1$
        \item Show that if $\ell_i = \lfloor -\log_2 p_i \rfloor$, then $\sum 2^{-\ell_i} \geq 1$
        \item Derive the optimal length assignment that minimizes $\sum p_i \ell_i$ subject to Kraft inequality
    \end{enumerate}

    \item \textbf{Code Design}:
    \begin{enumerate}
        \item Design an optimal prefix code for source with probabilities $\{0.25, 0.25, 0.2, 0.15, 0.1, 0.05\}$
        \item Calculate its expected length, efficiency, and redundancy
        \item Compare with Shannon code and Huffman code
    \end{enumerate}

    \item \textbf{Block Coding Analysis}:
    \begin{enumerate}
        \item For a binary source with $p=0.9$, design block codes for $n=1,2,3,4$
        \item Plot efficiency vs. block size
        \item Determine how large $n$ must be to achieve 90\% efficiency
    \end{enumerate}

    \item \textbf{Theoretical Limits}:
    \begin{enumerate}
        \item Prove that for a source with $m$ equally likely symbols, $L \geq \log_2 m$
        \item Show this is achievable with $\ell_i = \log_2 m$ for all $i$
        \item What happens when $\log_2 m$ is not an integer?
    \end{enumerate}

    \item \textbf{Research Extension}:
    \begin{enumerate}
        \item Investigate the concept of "minimum description length" (MDL)
        \item Compare with Shannon's approach
        \item Explain how MDL handles unknown distributions
    \end{enumerate}
\end{enumerate}

\subsection{Reading Assignment and References}
\begin{itemize}
    \item \textbf{Required Reading}:
    \begin{itemize}
        \item Cover \& Thomas, \textit{Elements of Information Theory}, Chapter 5: Sections 5.1-5.4
        \item Shannon, C. E. (1948). "A Mathematical Theory of Communication"
    \end{itemize}

    \item \textbf{Advanced References}:
    \begin{itemize}
        \item Gallager, R. G. (1968). \textit{Information Theory and Reliable Communication}
        \item Csiszár, I., \& Körner, J. (2011). \textit{Information Theory: Coding Theorems for Discrete Memoryless Systems}
    \end{itemize}

    \item \textbf{Historical Context}:
    \begin{itemize}
        \item Kraft, L. G. (1949). "A device for quantizing, grouping, and coding amplitude-modulated pulses"
        \item McMillan, B. (1956). "Two inequalities implied by unique decipherability"
    \end{itemize}
\end{itemize}

\subsection{Looking Ahead: Beyond Shannon's Theorem}
In the next lecture, we will explore:
\begin{itemize}
    \item \textbf{Huffman Coding}: Optimal prefix code construction algorithm
    \item \textbf{Arithmetic Coding}: Overcoming the integer length constraint
    \item \textbf{Universal Compression}: Coding without known statistics
    \item \textbf{Kolmogorov Complexity}: Algorithmic information theory perspective
\end{itemize}

\begin{importantbox}
\textbf{Key Theoretical Insights from Lecture 2}:
\begin{enumerate}
    \item Shannon's theorem establishes $H(X)$ as the fundamental limit for lossless compression
    \item The Kraft-McMillan inequality characterizes all uniquely decodable codes
    \item Block coding asymptotically achieves the entropy limit
    \item The "+1" overhead becomes negligible for high-entropy sources or large blocks
    \item Practical codes must balance optimality with complexity and delay constraints
\end{enumerate}
\end{importantbox}



\end{document}
