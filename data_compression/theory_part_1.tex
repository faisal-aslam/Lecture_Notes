\section{Shannon's Source Coding Theorem and Huffman Coding}

\subsubsection{Basic Terminology and Notation}

To avoid confusion, we clarify the fundamental terms used throughout
information theory and data compression.

\begin{definitionbox}
\textbf{Alphabet}

An \emph{alphabet} $\mathcal{X}$ is a finite set of possible symbols.
Examples:
\begin{itemize}
    \item Binary alphabet: $\mathcal{X} = \{0,1\}$
    \item English letters: $\mathcal{X} = \{\texttt{A},\dots,\texttt{Z}\}$
    \item Bytes: $\mathcal{X} = \{0,1,\dots,255\}$
\end{itemize}
\end{definitionbox}

\begin{definitionbox}
\textbf{Symbol}

A \emph{symbol} is a single element drawn from an alphabet.
For example, the letter \texttt{E} is a symbol from the English alphabet.
\end{definitionbox}

\begin{definitionbox}
\textbf{Random Variable}

A \emph{random variable} $X$ is a mathematical model of a source that produces symbols.
It assigns probabilities to symbols in the alphabet:
\[
P(X = x), \quad x \in \mathcal{X}
\]
Entropy is defined on random variables, not directly on symbols themselves.
\end{definitionbox}

\begin{definitionbox}
\textbf{Source}

A \emph{source} is a process that generates a sequence of symbols
$(X_1, X_2, X_3, \dots)$ according to some probability law.
In this lecture, we assume discrete sources unless stated otherwise.
\end{definitionbox}

\begin{definitionbox}
\textbf{Message (or Sequence)}

A \emph{message} is a finite sequence of symbols generated by the source:
\[
x^n = (x_1, x_2, \dots, x_n)
\]
Compression algorithms operate on messages, not on individual symbols.
\end{definitionbox}

\begin{definitionbox}
\textbf{Code and Codewords}

A \emph{code} assigns a binary string (codeword) to each symbol or message.
\begin{itemize}
    \item Source symbols $\rightarrow$ codewords (e.g., Huffman coding)
    \item Messages $\rightarrow$ bitstreams (e.g., arithmetic coding)
\end{itemize}
\end{definitionbox}

\begin{definitionbox}
\textbf{Block Length}

The \emph{block length} $n$ is the number of source symbols grouped together
and encoded as a unit. Larger block lengths generally allow better compression
but increase delay and complexity.
\end{definitionbox}

\begin{definitionbox}
\textbf{Model}

A \emph{model} estimates the probabilities of symbols or sequences.
Better models lead to better compression by reducing uncertainty.
\end{definitionbox}


\subsection{Information and Redundancy: The Core Concepts}

\subsubsection{Information: A Formal Measure of Uncertainty Reduction}

In common usage, ``information'' is often conflated with data, messages, or symbols. In \textbf{information theory}, information is defined rigorously as a \textbf{quantitative measure of the reduction in uncertainty} that results from observing the outcome of a random event.

The fundamental principle, established by Claude Shannon (1948), is:

\begin{quote}
    The information gained from an event is inversely related to its probability of occurrence. Highly probable events are unsurprising and convey little information; improbable events are surprising and convey substantial information.
\end{quote}

\begin{definition}
    Let $X$ be a random event that occurs with probability $p = \Pr(X)$. The \textbf{information content} (or \emph{self-information}) $I(X)$ provided by the occurrence of $X$ is defined as:
    \[
    I(X) = \log_b\left(\frac{1}{p}\right) = -\log_b(p)
    \]
    where:
    \begin{itemize}[noitemsep, topsep=2pt]
        \item The base $b$ of the logarithm determines the unit of information.
        \item $b=2$ yields \textbf{bits} (binary digits).
        \item $b=e$ yields \textbf{nats} (natural units).
        \item $b=10$ yields \textbf{hartleys} or \textbf{dits}.
    \end{itemize}
    The choice of base is a convention; the underlying concept is scale-invariant.
\end{definition}


This definition captures an important intuition: as an event becomes more predictable
($p \rightarrow 1$), its information content approaches zero.

\begin{examplebox}
\textbf{Predictability vs.\ Information}:
\begin{itemize}
    \item In a city where it rains every day, the statement ``It rained today'' conveys
    almost no information because it was already expected.
    \item A file that contains only the bit `1' provides very little information, since
    after seeing a few bits, the rest of the file can be predicted with certainty.
    \item A coin that always lands heads produces outcomes, but no information, because
    there is no uncertainty to resolve.
\end{itemize}

\textbf{Key idea}: Perfect predictability implies zero information gain.
\end{examplebox}

\begin{examplebox}
\textbf{Daily Weather Forecast — Information Content}:
\begin{itemize}
    \item Sunny in Phoenix (probability $0.9$):
    $I = -\log_2 0.9 \approx 0.15$ bits
    \item Snow in Phoenix (probability $0.001$):
    $I = -\log_2 0.001 \approx 9.97$ bits
    \item Rain in Seattle (probability $0.3$):
    $I = -\log_2 0.3 \approx 1.74$ bits
\end{itemize}

\textbf{Interpretation}: Rare events carry more information because they reduce uncertainty
the most. Snow in Phoenix reveals far more about the weather system than another sunny day.
\end{examplebox}

In summary, information is not about how much data is observed, but about how much
uncertainty is removed. This distinction between information and predictability forms
the foundation of redundancy, entropy, and data compression.

\subsubsection{Redundancy: The Enemy of Information and the Friend of Compression}

Redundancy refers to predictable or repeated structure in data.
From the perspective of information theory, redundancy is the \emph{enemy of information}
because it does not reduce uncertainty. However, from the perspective of data compression,
redundancy is a \emph{valuable resource}: it is precisely what allows data to be represented
using fewer bits.

Compression algorithms work by identifying, modeling, and removing redundancy while
preserving the underlying information (in lossless compression) or perceptually important
information (in lossy compression).

Redundancy appears in several common forms:

\begin{enumerate}
    \item \textbf{Spatial Redundancy}: Neighboring data values are highly correlated.
    \begin{examplebox}
    In a photograph of a clear blue sky, most neighboring pixels have nearly identical
    color values.
    \begin{itemize}
        \item \textbf{Naive}: Store the RGB value of each pixel independently.
        \item \textbf{Smarter}: Encode repeated pixel values using run-length encoding.
        \item \textbf{Even smarter}: Predict each pixel from its neighbors and encode
        only the small prediction error.
    \end{itemize}
    \end{examplebox}

    \item \textbf{Statistical Redundancy}: Some symbols occur far more frequently than others.
    \begin{examplebox}
    \textbf{English letter frequencies}:
    \begin{center}
    \begin{tabular}{|c|c||c|c|}
    \hline
    Letter & Frequency & Letter & Frequency \\
    \hline
    E & 12.7\% & Z & 0.07\% \\
    T & 9.1\% & Q & 0.10\% \\
    A & 8.2\% & J & 0.15\% \\
    \hline
    \end{tabular}
    \end{center}
    \begin{itemize}
        \item \textbf{Inefficient}: Fixed-length coding (5 bits per letter).
        \item \textbf{Efficient}: Variable-length coding (e.g., Huffman coding), where
        frequent letters get shorter codes.
    \end{itemize}
    This reduces the average bits per letter from 5 to approximately 4.1.
    \end{examplebox}

    \item \textbf{Knowledge Redundancy}: Information already known to both encoder and decoder.
    \begin{examplebox}
    \textbf{Medical Imaging}:
    Both the encoder and decoder know the image represents a chest X-ray.
    \begin{itemize}
        \item The general structure of lungs and bones does not need to be encoded explicitly.
        \item Anatomical models can be used to predict expected structures.
        \item Bits can be concentrated on unexpected or diagnostically important regions.
    \end{itemize}
    \end{examplebox}

    \item \textbf{Perceptual Redundancy}: Information that humans cannot perceive.
    \begin{examplebox}
    \textbf{Audio Compression (MP3)}:
    \begin{itemize}
        \item \textbf{Frequency masking}: Loud sounds mask nearby frequencies.
        \item \textbf{Temporal masking}: Loud sounds mask quieter sounds before or after them.
        \item \textbf{Result}: A large fraction of the audio data can be discarded without
        perceptible loss in quality.
    \end{itemize}
    \end{examplebox}
\end{enumerate}


\subsubsection{What is Entropy? Different Perspectives}

The term "entropy" appears in multiple fields (thermodynamics, information theory, statistics) with related but distinct meanings. In information theory, we primarily discuss \textbf{Shannon Entropy}, named after Claude Shannon who founded the field in 1948. While there are other entropy measures (like Kolmogorov-Sinai, Rényi, and Tsallis entropies in various contexts), Shannon entropy is the foundational concept for data compression.

\begin{definitionbox}
\textbf{Shannon Entropy} of a discrete random variable $X$ with possible values $\{x_1, x_2, \ldots, x_n\}$ having probabilities $\{p_1, p_2, \ldots, p_n\}$:
\[
H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i \quad \text{bits}
\]
\end{definitionbox}

\textbf{Two Complementary Interpretations}:

1. \textbf{Average Information Content}: When a symbol with probability $p_i$ occurs, it conveys $-\log_2 p_i$ bits of information (rare events tell us more). Entropy is the \textit{expected value} or average of this information content across all symbols.

2. \textbf{Uncertainty or Surprise}: Entropy measures how uncertain we are about the next symbol before observing it. Higher entropy means more unpredictability.

These interpretations are two sides of the same coin: \textit{The average information gained equals the uncertainty removed by observation.}

\subsubsection{Calculating Entropy: Step by Step}

Let's examine both interpretations through detailed calculations:

\begin{examplebox}
\textbf{Binary Source Example - Detailed Calculation}:

Consider a biased coin: P(Heads) = 0.8, P(Tails) = 0.2

\begin{enumerate}[label=\textbf{Step \arabic*}:]
    \item \textbf{Calculate individual information content}:
    \begin{align*}
        I_H &= -\log_2(0.8) \approx 0.3219 \text{ bits} \\
        I_T &= -\log_2(0.2) \approx 2.3219 \text{ bits}
    \end{align*}
    \textit{Interpretation}: Tails (rarer event) carries more information.
    
    \item \textbf{Calculate entropy as expected value}:
    \[
    H = 0.8 \times 0.3219 + 0.2 \times 2.3219 = 0.7219 \text{ bits}
    \]
    
    \item \textbf{Verify using direct formula}:
    \[
    H = -[0.8\log_2(0.8) + 0.2\log_2(0.2)] \approx 0.7219 \text{ bits}
    \]
\end{enumerate}

\textbf{Key Insights}:
\begin{itemize}
    \item \textbf{Average information}: Each flip gives 0.72 bits of information on average
    \item \textbf{Uncertainty}: We're 72\% as uncertain as with a fair coin
    \item \textbf{Extreme cases}:
    \begin{itemize}
        \item Fair coin (P=0.5): $H = 1.0$ bit (maximum uncertainty/information)
        \item Always heads (P=1.0): $H = 0$ bits (no uncertainty, no information)
        \item 90\% heads: $H \approx 0.469$ bits (less uncertainty than 80\% case)
    \end{itemize}
\end{itemize}
\end{examplebox}

\textbf{Mathematical Properties of Entropy}:

\begin{itemize}
    \item \textbf{Non-negativity}: $H(X) \geq 0$, with equality only when one outcome is certain
    \item \textbf{Maximum value}: For $n$ symbols, maximum entropy is $\log_2 n$, achieved when all probabilities are equal ($p_i = 1/n$)
    \item \textbf{Concavity}: Entropy is a concave function of probabilities
\end{itemize}

\subsubsection{Entropy of English Text: A Practical Case Study}

\begin{examplebox}
\textbf{Calculating English Letter Entropy}:

Based on letter frequencies in typical English text:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Letter} & \textbf{Probability ($p_i$)} & $\mathbf{-\log_2 p_i}$ & \textbf{Contribution ($p_i \times -\log_2 p_i$)} \\
\hline
E & 0.127 & 2.98 & 0.378 \\
T & 0.091 & 3.46 & 0.315 \\
A & 0.082 & 3.61 & 0.296 \\
O & 0.075 & 3.74 & 0.281 \\
I & 0.070 & 3.84 & 0.269 \\
N & 0.067 & 3.90 & 0.261 \\
S & 0.063 & 3.99 & 0.251 \\
H & 0.061 & 4.04 & 0.246 \\
R & 0.060 & 4.06 & 0.244 \\
D & 0.043 & 4.54 & 0.195 \\
\vdots & \vdots & \vdots & \vdots \\
Z & 0.0007 & 10.48 & 0.007 \\
\hline
\textbf{Total} & 1.0 & & \textbf{4.18 bits} \\
\hline
\end{tabular}
\end{center}

\textbf{Layered Interpretation}:
\begin{itemize}
    \item \textbf{First-order entropy (letters independent)}: 4.18 bits/letter
    \item \textbf{Why not 5 bits?} Because letters are not equally likely
    \item \textbf{Actual uncertainty is lower}: Letters have dependencies (Q is usually followed by U)
    
    \item \textbf{Comparison with encoding schemes}:
    \begin{center}
    \begin{tabular}{lcl}
    \toprule
    \textbf{Encoding Method} & \textbf{Bits/Letter} & \textbf{Efficiency} \\
    \midrule
    Naive (5 bits for 26 letters) & 5.00 & 83.6\% \\
    Huffman (letter-based) & 4.30 & 97.2\% \\
    Using digram frequencies & 3.90 & 107.2\%* \\
    Using word frequencies & 2.30 & 181.7\%* \\
    Optimal with full context & ~1.50 & 278.7\%* \\
    \bottomrule
    \end{tabular}
    \end{center}
    *Percentages $>$ 100\% show compression better than first-order entropy by exploiting dependencies
\end{itemize}
\end{examplebox}

\subsubsection{Beyond First-Order Entropy: The Full Picture}

Real-world data sources exhibit strong statistical dependencies between symbols.
To capture these dependencies, entropy is defined not only for single symbols,
but also for sequences of symbols.

\textbf{Higher-Order Entropies} quantify uncertainty while accounting for increasing context:

\begin{itemize}
    \item \textbf{Zero-order entropy ($H_0$)}:
    \[
    H_0 = \log_2 |\mathcal{X}|
    \]
    Assumes all symbols in the alphabet $\mathcal{X}$ are equally likely and independent.

    \item \textbf{First-order entropy ($H_1$)}:
    \[
    H_1 = -\sum_{x \in \mathcal{X}} p(x)\log_2 p(x)
    \]
    Accounts for symbol frequencies, but ignores dependencies between symbols.

    \item \textbf{Second-order entropy ($H_2$)}:
    \[
    H_2 = -\sum_{x,y} p(x,y)\log_2 p(x|y)
    \]
    Accounts for dependencies between adjacent symbol pairs.

    \item \textbf{$N$th-order entropy ($H_N$)}:
    \[
    H_N = -\sum p(x_1,\dots,x_N)\log_2 p(x_N \mid x_1,\dots,x_{N-1})
    \]
    Captures dependencies across blocks of length $N$.
\end{itemize}

As the context length increases, the entropy per symbol typically decreases,
reflecting increased predictability.

\subsubsection{Entropy Rate}

The \textbf{entropy rate} of a source is defined as the limiting uncertainty per symbol
when arbitrarily long contexts are available:

\[
H_\infty = \lim_{N \to \infty} H_N
\]

For stationary ergodic sources, this limit exists and characterizes the
\emph{true information content per symbol}.

\textbf{Example (English Text):}
\begin{itemize}
    \item First-order entropy: $\approx 4.0$ bits/letter
    \item Higher-order models reduce entropy significantly
    \item Estimated entropy rate: $\approx 1.0$--$1.5$ bits/letter
\end{itemize}

This large gap highlights the importance of modeling long-range dependencies.

\subsubsection{The Entropy Theorem: Why It Matters}

\begin{importantbox}
\textbf{Shannon's Source Coding Theorem (Informal and Formal Statement)}

Let a discrete memoryless source have entropy $H$.

\begin{enumerate}
    \item \textbf{Converse (Impossibility)}:
    No lossless coding scheme can achieve an average code length
    $L < H$ bits per symbol.

    \item \textbf{Achievability (Possibility)}:
    For any $\epsilon > 0$, there exists a coding scheme with
    \[
    H \le L < H + \epsilon
    \]
    for sufficiently large block sizes.
\end{enumerate}

\textbf{Key Implications for Compression}:
\begin{itemize}
    \item \textbf{Fundamental limit}: Entropy is a lower bound on achievable lossless compression
    \item \textbf{Optimality criterion}: A compression algorithm is good if $L$ is close to $H$
    \item \textbf{Redundancy}: Any excess over $H$ represents inefficiency
\end{itemize}
\end{importantbox}

\subsubsection{Practical Interpretation: English Text Compression}

\begin{itemize}
    \item \textbf{Impossible}: Average rate below the entropy rate ($\sim 1.0$--$1.5$ bits/letter)
    \item \textbf{Na\"ive encoding}: 8 bits/letter (ASCII)
    \item \textbf{Practical compression}: 2--3 bits/letter (modern context-based compressors)
    \item \textbf{Theoretical limit}: Entropy rate (100\% efficiency)
\end{itemize}

\subsubsection{Why the Entropy Limit Is Rarely Reached Exactly}

Even optimal algorithms cannot generally reach $H_\infty$ exactly due to:

\begin{itemize}
    \item Finite block lengths in practical implementations
    \item Computational and memory constraints
    \item Integer-length codeword restrictions (e.g., Huffman coding)
    \item Imperfect probabilistic models of the source
\end{itemize}

\subsubsection{Key Takeaways}

\begin{itemize}
    \item Entropy measures both \textbf{average information} and \textbf{uncertainty}
    \item Higher-order models reduce entropy by exploiting dependencies
    \item The entropy rate represents the ultimate compression limit
    \item Shannon's theorem precisely separates the \emph{possible} from the \emph{impossible}
    \item Good compression algorithms approach the entropy rate from above
\end{itemize}



\subsection{The Compression Pipeline: How Compressors Actually Work}
Most compressors follow this two-stage process:

% Simple text-based diagram
\begin{center}
\textbf{Compression Pipeline:}

\medskip
\begin{tabular}{cccc}
\textbf{Input Data} & $\longrightarrow$ & \textbf{Modeling Stage} & $\longrightarrow$ \\
& \scriptsize{Analyzes patterns} & & \scriptsize{Builds probability model} \\
\end{tabular}

\medskip
\begin{tabular}{cccc}
& $\longrightarrow$ & \textbf{Coding Stage} & $\longrightarrow$ \\
& & \scriptsize{Converts to bits} & \scriptsize{Using entropy coding} \\
\end{tabular}

\medskip
\begin{tabular}{cccc}
& $\longrightarrow$ & \textbf{Compressed Data} \\
\end{tabular}
\end{center}


\textbf{Two-Stage Compression Pipeline}:
\begin{itemize}
    \item \textbf{Modeling Stage}: Analyzes data patterns and builds probability model
    \item \textbf{Coding Stage}: Converts symbols to bits using entropy coding (Huffman, Arithmetic, ANS)
\end{itemize}


\subsection{Important Terminology and Concepts}
\subsubsection{Key Definitions with Examples}
\begin{itemize}
    \item \textbf{Symbol}: The basic unit being compressed
    \begin{examplebox}
    Different domains use different symbols:
    \begin{itemize}
        \item Text: Characters (bytes)
        \item Images: Pixels (RGB triples)
        \item Audio: Samples (16-bit integers)
        \item Video: Macroblocks (16$\times$16 pixel regions)
    \end{itemize}
    \end{examplebox}

    \item \textbf{Alphabet}: Set of all possible symbols
    \begin{examplebox}
    \begin{itemize}
        \item English text: 256 possible bytes (ASCII/UTF-8)
        \item Binary data: 256 possible byte values
        \item DNA sequences: 4 symbols \{A, C, G, T\}
        \item Black-white image: 2 symbols \{0=black, 1=white\}
    \end{itemize}
    \end{examplebox}

    \item \textbf{Prefix Code}: Crucial for instant decoding
    \begin{examplebox}
    \textbf{Why prefix codes matter}:
    \begin{itemize}
        \item Good: A=0, B=10, C=110, D=111
        \item "010110" decodes unambiguously: A(0) B(10) C(110)
        \item Bad: A=0, B=1, C=01 (not prefix-free)
        \item "01" could be AB or C - ambiguous!
    \end{itemize}
    \end{examplebox}
\end{itemize}

\subsubsection{The Fundamental Insight}
\begin{importantbox}
\textbf{The Core Principle of Compression}:
\begin{itemize}
    \item \textbf{Random data cannot be compressed}: Maximum entropy = no redundancy
    \item \textbf{Real-world data is not random}: Contains patterns, structure, predictability
    \item \textbf{Compression finds and exploits these patterns}

    \textbf{Example - Encryption vs Compression}:
    \begin{itemize}
        \item Encrypted data looks random (high entropy)
        \item Compressing encrypted data gives little or no savings
        \item Always compress \textbf{before} encrypting, not after!
        \item Rule: Encrypt $\rightarrow$ High entropy $\rightarrow$ No compression
        \item Rule: Compress $\rightarrow$ Lower entropy $\rightarrow$ Then encrypt
    \end{itemize}
\end{itemize}
\end{importantbox}




\begin{importantbox}
\textbf{Big Picture}:
\begin{itemize}
    \item \textbf{Entropy} = Theoretical limit of lossless compression
    \item \textbf{Kraft inequality} = Feasibility condition for prefix codes
    \item \textbf{Huffman coding} = Optimal prefix code construction
    \item \textbf{Block coding} = Approach entropy by coding symbols in blocks
    \item \textbf{Integer constraint} = Source of the "+1" overhead in Shannon's theorem
    \item \textbf{Arithmetic coding} = Removes integer constraint using fractional bits
\end{itemize}
\end{importantbox}

\begin{center}
\fbox{\begin{minipage}{0.9\textwidth}
\textbf{Three Key Coding Methods:}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Method} & \textbf{Purpose} & \textbf{Key Property} \\
\hline
Shannon coding & Proof of achievability & $\ell_i = \lceil -\log_2 p_i \rceil$ \\
Huffman coding & Optimal prefix code & Minimizes expected length \\
Arithmetic coding & Near-optimal compression & Fractional bits, removes integer constraint \\
\hline
\end{tabular}
\end{minipage}}
\end{center}

\subsection{Mathematical Preliminaries and Notation}
Let $X$ be a discrete random variable taking values in alphabet $\mathcal{X} = \{x_1, x_2, \ldots, x_m\}$ with probability mass function $p(x) = \Pr(X = x)$.

\begin{definitionbox}
\textbf{Source Code}: A mapping $C: \mathcal{X} \to \mathcal{D}^*$ where $\mathcal{D} = \{0, 1\}$ is the code alphabet, and $\mathcal{D}^*$ is the set of all finite binary strings. The code $C$ assigns to each symbol $x_i$ a codeword $c_i$ of length $\ell_i = |c_i|$.
\end{definitionbox}

\subsubsection{Expected Code Length}
For a source with probabilities $p_1, p_2, \ldots, p_m$ and corresponding codeword lengths $\ell_1, \ell_2, \ldots, \ell_m$, the expected code length is:
\[
L(C) = \mathbb{E}[\ell(X)] = \sum_{i=1}^m p_i \ell_i
\]

\subsection{Shannon's Source Coding Theorem: Formal Statement}
\begin{definitionbox}
\textbf{Shannon's Source Coding Theorem (1948)}:
\begin{itemize}
    \item \textbf{Converse (Lower Bound)}: For any uniquely decodable code $C$ for a discrete memoryless source $X$ with entropy $H(X)$, the expected length satisfies:
    \[
    L(C) \geq H(X)
    \]
    \item \textbf{Achievability (Upper Bound)}: There exists a uniquely decodable code $C$ such that:
    \[
    L(C) < H(X) + 1
    \]
    \item \textbf{Block Coding}: For the $n$th extension of the source, there exists a uniquely decodable code $C_n$ such that:
    \[
    \frac{1}{n} L(C_n) \to H(X) \quad \text{as } n \to \infty
    \]
\end{itemize}
\end{definitionbox}

\subsubsection{Interpretation and Significance}
\begin{itemize}
    \item \textbf{Fundamental Limit}: $H(X)$ bits/symbol is the \textbf{asymptotic lower bound} for lossless compression
    \item \textbf{Achievability}: We can get arbitrarily close to this limit by coding in blocks
    \item \textbf{Penalty Term}: The "+1" represents overhead from integer codeword lengths
    \item \textbf{The Integer Constraint}: Codeword lengths must be integers, but ideal lengths $-\log_2 p_i$ are typically not integers
    \item \textbf{Important}: The "+1" term is \textbf{not inefficiency of Huffman}, but a fundamental limitation of integer-length codes
\end{itemize}

\begin{examplebox}
\textbf{Binary Source Analysis}: Consider a binary source with $P(0) = p$, $P(1) = 1-p$:
\begin{itemize}
    \item Entropy: $H(p) = -p\log_2 p - (1-p)\log_2(1-p)$
    \item For $p = 0.1$: $H(0.1) \approx 0.469$ bits/symbol
    \item Theorem guarantees: $0.469 \leq L < 1.469$ bits/symbol
    \item Simple code: 0→0, 1→1 gives $L = 1$ bit/symbol (efficiency 46.9\%)
    \item Block coding can approach 0.469 bits/symbol
\end{itemize}
\end{examplebox}

\subsection{Code Classification and Properties}
\subsubsection{Hierarchical Classification of Codes}
\begin{center}
\textbf{Hierarchy of Codes:}

\medskip
\begin{tabular}{c}
All Codes \\
$\downarrow$ \\
Non-singular Codes \\
$\downarrow$ \\
Uniquely Decodable Codes \\
$\downarrow$ \\
Prefix Codes
\end{tabular}
\end{center}

\subsubsection{Formal Definitions}
\begin{enumerate}
    \item \textbf{Non-singular}: $C(x_i) \neq C(x_j)$ for $i \neq j$
    \item \textbf{Uniquely Decodable}: Every finite concatenation of codewords can be decoded in exactly one way
    \item \textbf{Prefix (Instantaneous)}: No codeword is a prefix of another
\end{enumerate}

\begin{importantbox}
\textbf{Hierarchy Theorem}: Every prefix code is uniquely decodable, but not vice versa. However, for every uniquely decodable code, there exists a prefix code with the same codeword lengths (McMillan's theorem).
\end{importantbox}

\begin{examplebox}
\textbf{Code Classification Examples}:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Code} & \textbf{Mapping} & \textbf{Singular?} & \textbf{Uniquely Decodable?} & \textbf{Prefix?} \\
\hline
$C_1$ & a→0, b→0, c→1 & Yes & No & No \\
\hline
$C_2$ & a→0, b→01, c→11 & No & No & No \\
\hline
$C_3$ & a→0, b→01, c→011 & No & No & No \\
\hline
$C_4$ & a→0, b→10, c→110 & No & Yes & Yes \\
\hline
\end{tabular}
\caption{Classification of different codes for alphabet $\{a,b,c\}$}
\end{table}

\textbf{Analysis of $C_3$}: The string "011" is ambiguous: it could be parsed as "ab" (0 followed by 11) or as "c" (011). Since there exists a string with multiple valid parsings, $C_3$ is not uniquely decodable.
\end{examplebox}

\subsection{Kraft-McMillan Inequality: Mathematical Foundation}
\begin{theorem}[Kraft-McMillan Inequality]
For any prefix code (or more generally, any uniquely decodable code) with codeword lengths $\ell_1, \ell_2, \ldots, \ell_m$ over a $D$-ary alphabet:
\[
\sum_{i=1}^m D^{-\ell_i} \leq 1
\]
where $D$ is the size of the code alphabet (2 for binary).
\end{theorem}

\subsubsection{Proof Sketch for Binary Prefix Codes}
\begin{enumerate}
    \item Consider a complete binary tree of depth $L = \max_i \ell_i$
    \item Each codeword of length $\ell_i$ occupies $2^{L-\ell_i}$ leaf positions
    \item Total occupied positions: $\sum_{i=1}^m 2^{L-\ell_i} \leq 2^L$
    \item Dividing by $2^L$: $\sum_{i=1}^m 2^{-\ell_i} \leq 1$
\end{enumerate}

\textbf{Note}: The tree-based proof above applies to prefix codes; the extension to uniquely decodable codes follows from McMillan's inequality.

\subsubsection{Converse: Constructing Codes from Lengths}
\begin{theorem}[Kraft Inequality Converse]
If integers $\ell_1, \ell_2, \ldots, \ell_m$ satisfy $\sum_{i=1}^m 2^{-\ell_i} \leq 1$, then there exists a binary prefix code with these lengths.
\end{theorem}

\begin{examplebox}
\textbf{Verifying Kraft Inequality}:
\begin{enumerate}
    \item Consider lengths $\{1, 2, 3, 3\}$:
    \[
    \sum 2^{-\ell_i} = 2^{-1} + 2^{-2} + 2^{-3} + 2^{-3} = 0.5 + 0.25 + 0.125 + 0.125 = 1
    \]
    A prefix code exists (e.g., 0, 10, 110, 111)

    \item Consider lengths $\{1, 1, 2\}$:
    \[
    \sum 2^{-\ell_i} = 2^{-1} + 2^{-1} + 2^{-2} = 0.5 + 0.5 + 0.25 = 1.25 > 1
    \]
    No prefix code exists with these lengths! This violates the Kraft inequality.
\end{enumerate}
\end{examplebox}

\subsection{Optimal Code Lengths and Shannon Coding}
\subsubsection{Shannon's Length Assignment}
For a source with probabilities $p_i$, Shannon proposed the length assignment:
\[
\ell_i = \lceil -\log_2 p_i \rceil
\]
where $\lceil x \rceil$ is the ceiling function.

\begin{theorem}
The lengths $\ell_i = \lceil -\log_2 p_i \rceil$ satisfy the Kraft inequality.
\end{theorem}

\begin{proof}
Since $\ell_i \geq -\log_2 p_i$, we have $-\ell_i \leq \log_2 p_i$, so:
\[
2^{-\ell_i} \leq p_i \quad \Rightarrow \quad \sum_{i=1}^m 2^{-\ell_i} \leq \sum_{i=1}^m p_i = 1
\]
\end{proof}

\begin{examplebox}
\textbf{Shannon Coding Example}: Source with probabilities $\{0.4, 0.3, 0.2, 0.1\}$
\begin{enumerate}
    \item Compute ideal lengths: $-\log_2 p_i = \{1.32, 1.74, 2.32, 3.32\}$
    \item Ceiling gives: $\ell_i = \{2, 2, 3, 4\}$
    \item Check Kraft: $2^{-2} + 2^{-2} + 2^{-3} + 2^{-4} = 0.25 + 0.25 + 0.125 + 0.0625 = 0.6875 \leq 1$
    \item Expected length: $L = 0.4\times2 + 0.3\times2 + 0.2\times3 + 0.1\times4 = 2.4$ bits/symbol
    \item Entropy: $H = 1.846$ bits/symbol
    \item Efficiency: $\eta = 1.846/2.4 = 76.9\%$
\end{enumerate}
\end{examplebox}

\subsection{Detailed Proof of Shannon's Theorem}
\subsubsection{Lower Bound: \protect$L \geq H(X)$}
\begin{proof}
Let $p_i$ be symbol probabilities and $\ell_i$ be codeword lengths of a uniquely decodable code. From Kraft-McMillan:
\[
\sum_{i=1}^m 2^{-\ell_i} \leq 1
\]
Define $r_i = 2^{-\ell_i} / \sum_{j=1}^m 2^{-\ell_j}$, so $\{r_i\}$ is a probability distribution.

Using the non-negativity of KL-divergence:
\[
D(p \| r) = \sum_{i=1}^m p_i \log_2 \frac{p_i}{r_i} \geq 0
\]
Substituting $r_i$:
\[
\sum_{i=1}^m p_i \log_2 p_i - \sum_{i=1}^m p_i \log_2 2^{-\ell_i} + \sum_{i=1}^m p_i \log_2 \left(\sum_{j=1}^m 2^{-\ell_j}\right) \geq 0
\]
Since $\sum_{j=1}^m 2^{-\ell_j} \leq 1$, the last term is $\leq 0$, giving:
\[
-H(X) + \sum_{i=1}^m p_i \ell_i \geq 0 \quad \Rightarrow \quad L \geq H(X)
\]
\end{proof}

\subsubsection{Upper Bound: \protect$L < H(X) + 1$}
\begin{proof}
Choose $\ell_i = \lceil -\log_2 p_i \rceil$. Then:
\[
-\log_2 p_i \leq \ell_i < -\log_2 p_i + 1
\]
Multiply by $p_i$ and sum over $i$:
\[
-\sum_{i=1}^m p_i \log_2 p_i \leq \sum_{i=1}^m p_i \ell_i < -\sum_{i=1}^m p_i \log_2 p_i + \sum_{i=1}^m p_i
\]
\[
H(X) \leq L < H(X) + 1
\]
\end{proof}

\subsubsection{The Integer Length Constraint}
The "+1" term in Shannon's theorem arises from the integer constraint on codeword lengths. For a \textbf{dyadic source} where all probabilities are of the form $p_i = 2^{-k_i}$ for integers $k_i$, we have $-\log_2 p_i = k_i$, which are integers. In this special case, we can achieve $L = H$ exactly.

\subsection{Extended Source Coding and Block Codes}
\subsubsection{The $n$th Extension of a Source}
For a discrete memoryless source $X$, the $n$th extension $X^n = (X_1, X_2, \ldots, X_n)$ has:
\[
H(X^n) = nH(X)
\]
Applying Shannon's theorem to $X^n$ gives a code $C_n$ with:
\[
nH(X) \leq L(C_n) < nH(X) + 1
\]
Thus, the average length per symbol satisfies:
\[
H(X) \leq \frac{L(C_n)}{n} < H(X) + \frac{1}{n}
\]

\begin{examplebox}
\textbf{Block Coding Improvement}: Binary source with $p(0) = 0.9$, $p(1) = 0.1$, $H = 0.469$ bits/symbol
\begin{itemize}
    \item Single symbol coding: Best code gives $L = 1$ bit/symbol (efficiency 46.9\%)
    \item \textbf{Block coding with $n=2$}: Consider coding pairs of symbols:
    \begin{align*}
        &00: (0.9)^2 = 0.81 \\
        &01: 0.9\times0.1 = 0.09 \\
        &10: 0.1\times0.9 = 0.09 \\
        &11: (0.1)^2 = 0.01
    \end{align*}
    \item Applying Shannon coding: $\ell_i = \lceil -\log_2 p_i \rceil$ gives lengths $\{1, 4, 4, 7\}$
    \item Expected length: $L_2 = 0.81\times1 + 0.09\times4 + 0.09\times4 + 0.01\times7 = 1.6$ bits/block
    \item Per symbol: $L_2/2 = 0.8$ bits/symbol (efficiency 58.6\%)
    \item \textbf{Note}: This Shannon coding is generally suboptimal compared to Huffman coding; we'll see better Huffman codes next.
    \item As $n \to \infty$: $L_n/n \to 0.469$ bits/symbol (100\% efficiency)
\end{itemize}
\end{examplebox}

\subsection{Code Efficiency and Redundancy Analysis}
\subsubsection{Performance Metrics}
\begin{definitionbox}
For a code $C$ with expected length $L$ coding a source with entropy $H$:
\[
\text{Efficiency: } \eta = \frac{H}{L} \times 100\% \quad \text{Redundancy: } \rho = L - H
\]
\end{definitionbox}

\subsubsection{Theoretical Bounds}
From Shannon's theorem:
\[
\frac{H}{H+1} \leq \eta \leq 1 \quad \text{and} \quad 0 \leq \rho < 1
\]

\begin{examplebox}
\textbf{Efficiency vs. Entropy}:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$H$ (bits/symbol) & Minimum $\eta$ & Maximum $\rho$ & Interpretation \\
\hline
0.1 & 9.1\% & 0.9 bits & Very compressible, but +1 term dominates \\
\hline
1.0 & 50\% & 1.0 bit & Worst-case Shannon bound for sources with $H=1$ \\
\hline
2.0 & 66.7\% & 1.0 bit & +1 becomes less significant \\
\hline
4.0 & 80\% & 1.0 bit & High entropy, good efficiency possible \\
\hline
7.0 & 87.5\% & 1.0 bit & +1 overhead relatively small \\
\hline
\end{tabular}
\caption{Theoretical limits on code efficiency for different entropy values}
\end{table}
\end{examplebox}

\subsection{Huffman Coding: Optimal Prefix Code Construction}

\subsubsection{The Huffman Algorithm: Step-by-Step}
\begin{theorem}[Huffman, 1952]
For a given source with symbol probabilities $p_1, p_2, \ldots, p_m$, the Huffman algorithm produces a prefix code that minimizes the expected code length $L = \sum p_i \ell_i$.
\end{theorem}

\begin{algorithm}[H]
\caption{Huffman Code Construction (Complete Version)}
\begin{algorithmic}[1]
\REQUIRE Symbols $x_1, \ldots, x_m$ with probabilities $p_1, p_2, \ldots, p_m$
\ENSURE Optimal binary prefix code
\STATE Create a min-priority queue $Q$ initialized with $m$ nodes, each containing one symbol and its probability
\WHILE{$|Q| > 1$}
    \STATE $a \gets \text{EXTRACT-MIN}(Q)$  \COMMENT{Node with smallest probability}
    \STATE $b \gets \text{EXTRACT-MIN}(Q)$  \COMMENT{Node with next smallest probability}
    \STATE Create new node $z$ with $p_z = p_a + p_b$
    \STATE Make $a$ left child of $z$, $b$ right child of $z$
    \STATE \text{INSERT}(Q, $z$)
\ENDWHILE
\STATE The remaining node in $Q$ is the root of the Huffman tree
\STATE Traverse tree from root to leaves, assigning 0 to left edges, 1 to right edges
\end{algorithmic}
\end{algorithm}

\begin{examplebox}
\textbf{Huffman Coding Example}: Source with probabilities:
\[
p(A) = 0.4,\quad p(B) = 0.3,\quad p(C) = 0.2,\quad p(D) = 0.1
\]

\begin{enumerate}
    \item \textbf{Step 1}: Combine C(0.2) and D(0.1) → CD(0.3)
    \item \textbf{Step 2}: Combine B(0.3) and CD(0.3) → BCD(0.6)
    \item \textbf{Step 3}: Combine A(0.4) and BCD(0.6) → Root(1.0)
    \item \textbf{Codes}: A=0 (1 bit), B=10 (2 bits), C=110 (3 bits), D=111 (3 bits)
\end{enumerate}

\textbf{Huffman Tree Visualization}:
\begin{center}
\footnotesize
\begin{tabular}{cccccc}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{Root (1.0)} & \multicolumn{2}{c}{} \\
\multicolumn{2}{c}{} & 0 & 1 & \multicolumn{2}{c}{} \\
\multicolumn{2}{c}{A (0.4)} & \multicolumn{3}{c}{BCD (0.6)} & \\
 & & 0 & \multicolumn{2}{c}{1} & \\
 & & B (0.3) & \multicolumn{2}{c}{CD (0.3)} & \\
 & & & 0 & 1 & \\
 & & & C (0.2) & D (0.1) &
\end{tabular}
\end{center}

\textbf{Analysis}:
\begin{itemize}
    \item Expected length: $L = 0.4\times1 + 0.3\times2 + 0.2\times3 + 0.1\times3 = 1.9$ bits/symbol
    \item Entropy: $H = 1.846$ bits/symbol
    \item Efficiency: $\eta = 1.846/1.9 = 97.1\%$
    \item Shannon code (from earlier): $L = 2.4$ bits, $\eta = 76.9\%$
    \item \textbf{Huffman is significantly better than Shannon coding!}
\end{itemize}
\end{examplebox}

\subsubsection{Huffman vs. Shannon's Theorem}
\begin{itemize}
    \item \textbf{Shannon's Theorem}: Proves $H \leq L < H + 1$ is achievable
    \item \textbf{Shannon Coding}: Simple construction with $\ell_i = \lceil -\log_2 p_i \rceil$
    \item \textbf{Huffman Coding}: Optimal construction that minimizes $L$
    \item \textbf{Relationship}: $L_{\text{Huffman}} \leq L_{\text{Shannon}} \leq H + 1$
    \item \textbf{For dyadic sources}: Both achieve $L = H$ exactly
\end{itemize}

\subsubsection{Huffman Code Properties}
\begin{theorem}[Huffman Code Length Bound]
For a Huffman code with codeword lengths $\ell_i$, each length satisfies:
\[
\ell_i \leq \lceil -\log_2 p_i \rceil
\]
That is, no Huffman codeword is longer than the corresponding Shannon codeword.
\end{theorem}

\begin{itemize}
    \item \textbf{Optimality}: Minimizes expected code length among all prefix codes
    \item \textbf{Uniqueness}: \textbf{Not unique in general}. If all probabilities are distinct and no ties occur during merging, the code lengths are unique up to relabeling; otherwise multiple optimal trees may exist.
    \item \textbf{Length bound}: $\ell_i \leq m-1$ for $m$ symbols
    \item \textbf{Two least probable}: Always have same length, differ only in last bit
    \item \textbf{Kraft inequality}: Huffman codes satisfy $\sum_{i=1}^m 2^{-\ell_i} \leq 1$, with equality if and only if the tree is complete
\end{itemize}

\subsubsection{Optimal Code Structure Lemma}
\begin{lemma}
In an optimal prefix code:
\begin{enumerate}
    \item If $p_j > p_k$, then $\ell_j \leq \ell_k$ (more probable = shorter code)
    \item The two least probable symbols have the same length
    \item The two least probable symbols differ only in the last bit
\end{enumerate}
\end{lemma}

\subsubsection{Optimality Proof of Huffman Coding}
\begin{proof}[Proof Sketch]
By induction on number of symbols $m$:

\textbf{Base case} ($m=2$): Trivial - need exactly 1 bit per symbol.

\textbf{Inductive step}: Assume Huffman optimal for $m-1$ symbols.

Let $x$ and $y$ be two least probable symbols. Consider reduced alphabet where $x$ and $y$ are merged into $z$ with $p_z = p_x + p_y$.

\begin{enumerate}
    \item If $C'$ is optimal for reduced alphabet, then creating $C$ by splitting $z$ into $x$ and $y$ (appending 0 and 1) gives:
    \[
    L(C) = L(C') + p_x + p_y
    \]

    \item Any optimal code for original alphabet must have $x$ and $y$ as siblings (same parent)

    \item Huffman algorithm finds such sibling pairing

    \item By induction hypothesis, $C'$ is optimal for reduced alphabet

    \item Therefore $C$ is optimal for original alphabet
\end{enumerate}
\end{proof}

\subsubsection{Redundancy Bound}
\begin{theorem}[Gallager's Redundancy Bound, 1978]
For a Huffman code, the redundancy is bounded by:
\[
L - H < p_{\text{min}} + 0.086
\]
where $p_{\text{min}}$ is the smallest symbol probability. Moreover:
\begin{itemize}
    \item This bound is tight (achievable for some distributions)
    \item For dyadic sources ($p_i = 2^{-k_i}$): $L = H$ exactly
    \item Worst-case redundancy occurs when $p_{\text{min}} \approx 0$
\end{itemize}
\end{theorem}

\begin{examplebox}
\textbf{Redundancy Analysis}:
\begin{itemize}
    \item \textbf{Best-case}: For dyadic source with $p_i = 2^{-k_i}$, $L = H$ exactly
    \item \textbf{Worst-case}: When smallest probability is very small
    \item \textbf{Example}: For source with probabilities $\{0.999, 0.001\}$:
    \begin{itemize}
        \item Huffman codes: 0→0, 1→1 (1 bit each)
        \item $L = 1$ bit/symbol, $H \approx 0.011$ bits/symbol
        \item Redundancy: $\rho \approx 0.989$ bits/symbol
        \item Efficiency: $\eta \approx 1.1\%$
    \end{itemize}
\end{itemize}
\end{examplebox}

\subsection{Extended Huffman Coding: Approaching the Entropy Limit}

\subsubsection{The Problem with Basic Huffman}
Even optimal Huffman coding suffers from the "$+1$" term in Shannon's theorem:
\[
L < H(X) + 1
\]
For low-entropy sources, this overhead is significant:
\begin{examplebox}
\textbf{Binary source with $p(0)=0.9$, $p(1)=0.1$}:
\begin{itemize}
    \item Entropy: $H(0.9) = 0.469$ bits/symbol
    \item Basic Huffman: Symbols \{0,1\}, codes \{0,1\}, $L = 1$ bit/symbol
    \item Efficiency: $\eta = 46.9\%$ (poor!)
    \item Problem: Overhead 0.531 bits/symbol > entropy itself!
\end{itemize}
\end{examplebox}

\subsubsection{Block Huffman Coding Solution}
Code $n$ symbols together as "super-symbols":

\begin{enumerate}
    \item Consider $n$th extension of source: $X^n = (X_1, X_2, \ldots, X_n)$
    \item Alphabet size grows to $m^n$ sequences
    \item Apply Huffman coding to these blocks
\end{enumerate}

\begin{theorem}[Extended Huffman Performance]
For the $n$th extension coded with Huffman, the average length per symbol satisfies:
\[
\lim_{n\to\infty} \frac{L_n}{n} = H(X)
\]
\end{theorem}

\textbf{Note}: While Huffman coding typically performs at least as well as Shannon coding, it lacks a universal closed-form finite-$n$ bound like $L_n/n < H + 1/n$.

\begin{examplebox}
\textbf{Extended Huffman for Binary Source $p(0)=0.9$, $p(1)=0.1$}:

\textbf{$n=2$ blocks (4 sequences)}:
\begin{itemize}
    \item Probabilities: $P(00)=0.81$, $P(01)=0.09$, $P(10)=0.09$, $P(11)=0.01$
    \item Huffman codes: 00→0, 01→10, 10→110, 11→111
    \item Expected length: $L_2 = 0.81\times1 + 0.09\times2 + 0.09\times3 + 0.01\times3 = 1.29$ bits/block
    \item Per symbol: $L_2/2 = 0.645$ bits/symbol
    \item Efficiency: $\eta = 0.469/0.645 = 72.7\%$ (vs 46.9\% for $n=1$)
\end{itemize}

\textbf{$n=3$ blocks (8 sequences)}:
\begin{itemize}
    \item Probabilities:
    \begin{align*}
        &000: 0.729, \quad 001: 0.081, \quad 010: 0.081, \quad 011: 0.009 \\
        &100: 0.081, \quad 101: 0.009, \quad 110: 0.009, \quad 111: 0.001
    \end{align*}
    \item \textbf{Note}: Showing optimality for large $n$ is complex. The following is illustrative, not proven optimal.
    \item One possible Huffman assignment:
    \begin{align*}
        &000 \to 0, \quad 001 \to 100, \quad 010 \to 101, \quad 011 \to 11000 \\
        &100 \to 1101, \quad 101 \to 11001, \quad 110 \to 1110, \quad 111 \to 1111
    \end{align*}
    \item Expected length: $L_3 \approx 1.6$ bits/block
    \item Per symbol: $L_3/3 \approx 0.533$ bits/symbol
    \item Efficiency: $\eta = 0.469/0.533 \approx 88.0\%$
\end{itemize}

\textbf{Trend}: As $n$ increases, $L_n/n \to H(X)$
\end{examplebox}

\subsubsection{Practical Issues with Block Huffman}
\begin{itemize}
    \item \textbf{Exponential complexity}: Alphabet size grows as $m^n$
    \item \textbf{Memory requirements}: Need to store $2m^n-1$ nodes in Huffman tree
    \item \textbf{Delay}: Must wait for $n$ symbols before encoding/decoding
    \item \textbf{Adaptation}: Probabilities may change over time
\end{itemize}

\begin{importantbox}
\textbf{Warning: Theoretical vs. Practical Use of Block Huffman}
Block Huffman coding demonstrates that we can approach the entropy limit arbitrarily closely, but it is **of theoretical interest only for large $n$**. The exponential growth in complexity makes it impractical for real applications with large alphabets or large block sizes.
\end{importantbox}

\begin{examplebox}
\textbf{Complexity Growth for English Text ($m=256$)}:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$n$ & \textbf{Block Size} & \textbf{\# Sequences} & \textbf{Tree Nodes} \\
\hline
1 & 1 byte & 256 & 511 \\
2 & 2 bytes & 65,536 & 131,071 \\
3 & 3 bytes & 16.7 million & 33.5 million \\
4 & 4 bytes & 4.3 billion & 8.6 billion \\
5 & 5 bytes & 1.1 trillion & 2.2 trillion \\
\hline
\end{tabular}
\caption{Exponential growth makes large $n$ impractical}
\end{table}
\end{examplebox}

\subsection{Adaptive Huffman Coding: Real-time Solution}

\subsubsection{The FGK Algorithm (Faller-Gallager-Knuth)}
\begin{itemize}
    \item \textbf{Dynamic}: Updates code as symbols arrive
    \item \textbf{No initial model}: Learns probabilities on-the-fly
    \item \textbf{Sibling property}: Maintains optimality after each update
    \item \textbf{Complexity}: $O(m)$ worst-case per update; can be reduced with careful data structures
    \item \textbf{Applications}: Used in early versions of UNIX compress, modem protocols
\end{itemize}

\subsubsection{Comparison: Static vs Adaptive vs Block}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Efficiency} & \textbf{Complexity} & \textbf{Delay} \\
\hline
Static Huffman & High if model good & $O(m \log m)$ & None \\
Adaptive Huffman & Medium-High & $O(m)$ per symbol & None \\
Block Huffman ($n$) & Approaches optimal & $O(m^n \log m^n)$ & $n$ symbols \\
\hline
\end{tabular}
\caption{Trade-offs in Huffman coding variants}
\end{table}

\begin{importantbox}
\textbf{Why Arithmetic Coding Beats Huffman}
Huffman coding is limited by the **integer constraint** on codeword lengths, leading to the "+1" overhead in Shannon's theorem. Arithmetic coding, which we'll cover next, uses **fractional bits** and can achieve average lengths arbitrarily close to $H(X)$ without block coding, removing the "+1" penalty entirely.
\end{importantbox}

\subsection{Practical Implications and Limitations}
\subsubsection{Assumptions of Shannon's Theorem}
\begin{itemize}
    \item \textbf{Discrete Memoryless Source}: Symbols independent and identically distributed
    \item \textbf{Known Distribution}: Probabilities $p_i$ are known in advance
    \item \textbf{Arbitrary Delay}: Block coding allows infinite delay for encoding/decoding
    \item \textbf{No Complexity Constraints}: No limits on computational resources
\end{itemize}

\subsubsection{Violations in Practice}
\begin{examplebox}
\textbf{Real-world Violations}:
\begin{itemize}
    \item \textbf{Dependencies}: English text has strong correlations between letters
    \item \textbf{Unknown Distribution}: Must estimate probabilities from data
    \item \textbf{Delay Constraints}: Real-time applications limit block size
    \item \textbf{Complexity}: Exponential growth with block size ($m^n$ sequences)
    \item \textbf{Solution Approaches}: Adaptive coding, dictionary methods (LZ), arithmetic coding
\end{itemize}
\end{examplebox}

\subsection{Extensions and Generalizations}
\subsubsection{Markov Sources}
For a $k$th order Markov source with conditional entropy $H(X|X^{k})$, the theorem extends to:
\[
H(X|X^{k}) \leq L < H(X|X^{k}) + 1
\]

\subsubsection{Universal Coding}
When the source distribution is unknown, universal codes achieve:
\[
\frac{1}{n} L_n \to H(X) \quad \text{almost surely}
\]
Examples: Lempel-Ziv codes, arithmetic coding with adaptive models.

\subsubsection{Rate-Distortion Theory}
For lossy compression with distortion $D$, the rate-distortion function $R(D)$ gives the minimum achievable rate:
\[
R(D) = \min_{p(\hat{x}|x): \mathbb{E}[d(X,\hat{X})] \leq D} I(X;\hat{X})
\]

\subsection{Advanced Examples and Applications}
\begin{examplebox}
\textbf{DNA Sequence Compression}: Alphabet $\{A,C,G,T\}$ with typical probabilities $\{0.3, 0.2, 0.2, 0.3\}$
\begin{itemize}
    \item Entropy: $H = 1.97$ bits/base
    \item Simple code: 2 bits/base (efficiency 98.5\%)
    \item Exploiting dependencies: Adjacent bases are correlated in genomes
    \item Conditional entropy: $H(X_n|X_{n-1}) \approx 1.5$ bits/base
    \item Practical compressors achieve ~1.6 bits/base
\end{itemize}
\end{examplebox}

\begin{examplebox}
\textbf{Image Compression Limit}: Grayscale image with 256 levels
\begin{itemize}
    \item Naive: 8 bits/pixel
    \item Actual entropy from pixel correlations: Typically 1-4 bits/pixel
    \item PNG (lossless): 2-6 bits/pixel (uses filtering + DEFLATE = LZ77 + Huffman)
    \item JPEG (lossy): 0.5-2 bits/pixel with visual quality
    \item Theoretical limit from image statistics
\end{itemize}
\end{examplebox}

\subsection*{Historical Notes}
\begin{itemize}
    \item \textbf{1948}: Claude Shannon publishes "A Mathematical Theory of Communication"
    \item \textbf{1949}: Leon Kraft proves inequality for prefix codes
    \item \textbf{1952}: David Huffman, as a graduate student at MIT, invents optimal prefix coding algorithm
    \item \textbf{1956}: Brockway McMillan extends Kraft inequality to all uniquely decodable codes
    \item \textbf{1978}: Robert Gallager proves tight redundancy bound for Huffman codes
\end{itemize}

\subsection*{Homework Assignment 2: Source Coding Theory and Huffman Coding}

\textbf{Note}: Questions 2 and 3 are comprehensive problems. Question 3 is a challenge problem that explores block coding in depth.

\begin{enumerate}
    \item \textbf{Kraft-McMillan Applications (20 points)}
    \begin{enumerate}
        \item Prove that for any uniquely decodable code with codeword lengths $\ell_1, \ell_2, \ldots, \ell_m$, we have $\sum_{i=1}^m 2^{-\ell_i} \leq 1$
        \item Given lengths $\{2, 3, 3, 3, 4, 4\}$, verify if a binary prefix code exists
        \item If a code exists, construct it using the canonical method
    \end{enumerate}

    \item \textbf{Huffman Code Construction (30 points)}
    \begin{enumerate}
        \item For source with probabilities $\{0.35, 0.2, 0.15, 0.1, 0.1, 0.05, 0.05\}$:
        \begin{itemize}
            \item Construct Huffman tree step by step (show all intermediate steps)
            \item Assign codewords and calculate expected length
            \item Compute efficiency and redundancy
        \end{itemize}
        \item Using the lemma from class, prove that in your Huffman code, the two least probable symbols have codewords of equal length
        \item Verify that your code satisfies the Kraft inequality (note: it may not satisfy it with equality if the tree is not complete)
    \end{enumerate}

    \item \textbf{Extended Huffman Coding: Challenge Problem (25 points)}
    \begin{enumerate}
        \item Consider a binary source with $p(0)=0.8$, $p(1)=0.2$
        \item Design Huffman codes for $n=1,2,3$ (code blocks of symbols together)
        \item For each $n$, calculate:
        \begin{itemize}
            \item Expected length per block $L_n$
            \item Expected length per symbol $L_n/n$
            \item Efficiency $\eta_n$
        \end{itemize}
        \item Plot $L_n/n$ vs $n$ (for $n=1,2,3$) and compare to the entropy limit
        \item Explain mathematically why efficiency improves with $n$
    \end{enumerate}

    \item \textbf{Optimality Analysis (15 points)}
    \begin{enumerate}
        \item For the source in question 2, design a Shannon code ($\ell_i = \lceil -\log_2 p_i \rceil$)
        \item Compare the Huffman code with the Shannon code:
        \begin{itemize}
            \item Expected lengths
            \item Efficiencies
            \item Redundancies
        \end{itemize}
        \item Under what conditions (what type of probability distributions) does Huffman coding achieve exactly $L = H$?
    \end{enumerate}

    \item \textbf{Practical Considerations (10 points)}
    \begin{enumerate}
        \item Explain why block Huffman coding with large $n$ is impractical for real applications
        \item What alternative approaches exist for approaching the entropy limit without exponential complexity?
        \item How does arithmetic coding (to be covered in the next lecture) solve the integer constraint problem?
    \end{enumerate}
\end{enumerate}

\subsection*{Reading Assignment and References}
\begin{itemize}
    \item \textbf{Required Reading}:
    \begin{itemize}
        \item Cover \& Thomas, \textit{Elements of Information Theory}, Chapter 5: Data Compression
        \item Shannon, C. E. (1948). "A Mathematical Theory of Communication"
    \end{itemize}

    \item \textbf{Advanced References}:
    \begin{itemize}
        \item Huffman, D. A. (1952). "A Method for the Construction of Minimum-Redundancy Codes"
        \item Gallager, R. G. (1978). "Variations on a Theme by Huffman"
    \end{itemize}

    \item \textbf{Online Resources}:
    \begin{itemize}
        \item Visual Huffman tree generator: \url{https://www.csfieldguide.org.nz/en/interactives/huffman-tree/}
        \item Kraft inequality calculator: \url{https://planetcalc.com/8157/}
    \end{itemize}
\end{itemize}
