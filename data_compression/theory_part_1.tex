\section{Theory of Compression — Limits and Optimality}

\subsection{Types of Codes: From Ambiguous to Instantaneous}

\begin{definitionbox}
\textbf{Types of Codes}

\begin{itemize}
    \item \textbf{Non-singular Code}: Each source symbol maps to a distinct codeword
    \[
    x_i \neq x_j \Rightarrow C(x_i) \neq C(x_j)
    \]
    
    \item \textbf{Uniquely Decodable Code}: Every finite sequence of codewords corresponds to exactly one sequence of source symbols
    \[
    C(x_1)C(x_2)\cdots C(x_n) = C(y_1)C(y_2)\cdots C(y_m) \Rightarrow n=m \text{ and } x_i = y_i
    \]
    
    \item \textbf{Prefix Code (Instantaneous Code)}: No codeword is a prefix of another codeword
    \[
    \forall i \neq j: \quad C(x_i) \text{ is not a prefix of } C(x_j)
    \]
\end{itemize}

\textbf{Key Relationships}:
\[
\text{Prefix Codes} \subset \text{Uniquely Decodable Codes} \subset \text{Non-singular Codes}
\]
\end{definitionbox}

\begin{importantbox}
\textbf{Why Prefix Codes are Special}
\begin{itemize}
    \item \textbf{Instantaneous decoding}: Can decode as soon as codeword ends (no lookahead needed)
    \item \textbf{Tree representation}: Always correspond to leaves of a binary tree
    \item \textbf{Kraft inequality}: Always satisfy $\sum 2^{-\ell_i} \leq 1$
    \item \textbf{Practical}: Used in Huffman coding, many real-world compressors
\end{itemize}
\end{importantbox}

\begin{examplebox}
\textbf{Example: Comparing Different Code Types}

For symbols $\{A,B,C,D\}$ with probabilities $\{0.5,0.25,0.125,0.125\}$:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Code Type} & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} & \textbf{Property} \\
\hline
Non-singular & 0 & 1 & 00 & 11 & Distinct but ambiguous: "00" = AA or C? \\
\hline
Uniquely decodable & 0 & 01 & 011 & 0111 & Unique but need lookahead \\
\hline
Prefix code & 0 & 10 & 110 & 111 & Instant decoding: "0" = A, stop \\
\hline
Optimal prefix & 0 & 10 & 110 & 111 & Also Huffman optimal \\
\hline
\end{tabular}
\end{center}

\textbf{Decoding examples}:
\begin{itemize}
    \item \textbf{Prefix code "010110"}: 0→A, 10→B, 110→C = "ABC" (instant)
    \item \textbf{Uniquely decodable "00111"}: Need to scan ahead to determine split
    \item \textbf{Non-singular "00"}: Ambiguous! Could be "AA" or "C"
\end{itemize}

\textbf{Key insight}: Prefix codes sacrifice some flexibility in codeword lengths (must satisfy Kraft inequality) for the benefit of instantaneous decoding.
\end{examplebox}


\subsection{Basic Terminology and Notation}

\begin{definitionbox}
\textbf{Alphabet}

An \emph{alphabet} $\mathcal{X}$ is a finite set of possible symbols.
Examples:
\begin{itemize}
    \item Binary alphabet: $\mathcal{X} = \{0,1\}$
    \item English letters: $\mathcal{X} = \{\texttt{A},\dots,\texttt{Z}\}$
    \item Bytes: $\mathcal{X} = \{0,1,\dots,255\}$
\end{itemize}
\end{definitionbox}

\begin{definitionbox}
\textbf{Symbol}

A \emph{symbol} is a single element drawn from an alphabet.
For example, the letter \texttt{E} is a symbol from the English alphabet.
\end{definitionbox}

\begin{definitionbox}
\textbf{Random Variable}

A \emph{random variable} $X$ is a function that assigns a symbol or value
to each outcome in a sample space:

\[
X: \Omega \to \mathcal{X}
\]

\begin{itemize}
    \item $\Omega$: Sample space (e.g., all possible states of a data source)
    \item $\mathcal{X}$: Set of possible values (alphabet, e.g., $\{0,1\}$, ASCII characters)
    \item For each $\omega \in \Omega$, $X(\omega)$ is the value assigned to outcome $\omega$
\end{itemize}

\textbf{Example: Binary Source}
\begin{itemize}
    \item $\Omega = \{\text{emits 0}, \text{emits 1}\}$ (or could be more complex underlying physics)
    \item $\mathcal{X} = \{0, 1\}$
    \item $X(\text{emits 0}) = 0$, $X(\text{emits 1}) = 1$
    \item Probabilities: $P(X=0) = P(\{\omega: X(\omega)=0\}) = p$, $P(X=1) = 1-p$
\end{itemize}

\textbf{Why this matters for compression}:
\begin{itemize}
    \item The entropy $H(X)$ depends on the probability distribution induced by $X$
    \item For $x \in \mathcal{X}$: $P(X=x) = P(\{\omega \in \Omega: X(\omega)=x\})$
    \item $H(X) = -\sum_{x \in \mathcal{X}} P(X=x) \log_2 P(X=x)$
\end{itemize}
\end{definitionbox}

\begin{definitionbox}
\textbf{Source}

A \emph{source} is a process that generates a sequence of symbols
$(X_1, X_2, X_3, \dots)$ according to some probability law.
In this lecture, we assume discrete sources unless stated otherwise.
\end{definitionbox}

\begin{definitionbox}
\textbf{Message (or Sequence)}

A \emph{message} is a finite sequence of symbols generated by the source:
\[
x^n = (x_1, x_2, \dots, x_n)
\]
Compression algorithms operate on messages, not on individual symbols.
\end{definitionbox}

\begin{definitionbox}
\textbf{Code and Codewords}

A \emph{code} assigns a binary string (codeword) to each symbol or message.
\begin{itemize}
    \item Source symbols $\rightarrow$ codewords (e.g., Huffman coding)
    \item Messages $\rightarrow$ bitstreams (e.g., arithmetic coding)
\end{itemize}
\end{definitionbox}

\begin{definitionbox}
\textbf{Block Length}

The \emph{block length} $n$ is the number of source symbols grouped together
and encoded as a unit. Larger block lengths generally allow better compression
but increase delay and complexity.
\end{definitionbox}

\begin{definitionbox}
\textbf{Model}

A \emph{model} estimates the probabilities of symbols or sequences.
Better models lead to better compression by reducing uncertainty.
\end{definitionbox}

\subsection{Information and Redundancy: The Core Concepts}

\subsubsection{Information: A Formal Measure of Uncertainty Reduction}

In information theory, information is defined rigorously as a \textbf{quantitative measure of the reduction in uncertainty} that results from observing the outcome of a random event.

\begin{definition}
    Let $X$ be a random event that occurs with probability $p = \Pr(X)$. The \textbf{information content} (or \emph{self-information}) $I(X)$ provided by the occurrence of $X$ is defined as:
    \[
    I(X) = \log_b\left(\frac{1}{p}\right) = -\log_b(p)
    \]
    where:
    \begin{itemize}[noitemsep, topsep=2pt]
        \item $b=2$ yields \textbf{bits} (binary digits)
        \item $b=e$ yields \textbf{nats} (natural units)
        \item $b=10$ yields \textbf{hartleys} or \textbf{dits}
    \end{itemize}
\end{definition}

\begin{examplebox}
\textbf{Predictability vs.\ Information}:
\begin{itemize}
    \item In a city where it rains every day, the statement ``It rained today'' conveys almost no information because it was expected
    \item A file that contains only the bit `1' provides very little information
    \item A coin that always lands heads produces outcomes, but no information
    
\textbf{Key idea}: Perfect predictability implies zero information gain.
\end{itemize}
\end{examplebox}

\begin{examplebox}
\textbf{Daily Weather Forecast — Information Content}:
\begin{itemize}
    \item Sunny in Phoenix (probability $0.9$): $I = -\log_2 0.9 \approx 0.15$ bits
    \item Snow in Phoenix (probability $0.001$): $I = -\log_2 0.001 \approx 9.97$ bits
    \item Rain in Seattle (probability $0.3$): $I = -\log_2 0.3 \approx 1.74$ bits
\end{itemize}
\textbf{Interpretation}: Rare events carry more information because they reduce uncertainty the most.
\end{examplebox}

\subsubsection{Redundancy: The Enemy of Information and the Friend of Compression}

Redundancy refers to predictable or repeated structure in data. It is what allows data to be represented using fewer bits.

\begin{enumerate}
    \item \textbf{Spatial Redundancy}: Neighboring data values are highly correlated
    \begin{examplebox}
    In a photograph of a clear blue sky, most neighboring pixels have nearly identical color values.
    \begin{itemize}
        \item \textbf{Naive}: Store the RGB value of each pixel independently
        \item \textbf{Smarter}: Encode repeated pixel values using run-length encoding
        \item \textbf{Even smarter}: Predict each pixel from its neighbors and encode only the small prediction error
    \end{itemize}
    \end{examplebox}
    
    \item \textbf{Statistical Redundancy}: Some symbols occur far more frequently than others
    \begin{examplebox}
    \textbf{English letter frequencies}:
    \begin{center}
    \begin{tabular}{|c|c||c|c|}
    \hline
    Letter & Frequency & Letter & Frequency \\
    \hline
    E & 12.7\% & Z & 0.07\% \\
    T & 9.1\% & Q & 0.10\% \\
    A & 8.2\% & J & 0.15\% \\
    \hline
    \end{tabular}
    \end{center}
    Frequent letters get shorter codes in variable-length coding schemes.
    \end{examplebox}
    
    \item \textbf{Knowledge Redundancy}: Information already known to both encoder and decoder
    \item \textbf{Perceptual Redundancy}: Information that humans cannot perceive
\end{enumerate}

\subsection{Entropy: The Fundamental Limit}

\subsubsection{What is Entropy? Different Perspectives}

\begin{definitionbox}
\textbf{Shannon Entropy} of a discrete random variable $X$ with possible values $\{x_1, x_2, \ldots, x_n\}$ having probabilities $\{p_1, p_2, \ldots, p_n\}$:
\[
H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i \quad \text{bits}
\]
\end{definitionbox}

\textbf{Two Complementary Interpretations}:
\begin{enumerate}
    \item \textbf{Average Information Content}: Expected value of information content across all symbols
    \item \textbf{Uncertainty or Surprise}: Measures how uncertain we are about the next symbol
\end{enumerate}

\subsubsection{Calculating Entropy: Step by Step}

\begin{examplebox}
\textbf{Binary Source Example - Detailed Calculation}:

Consider a biased coin: P(Heads) = 0.8, P(Tails) = 0.2

\begin{enumerate}[label=\textbf{Step \arabic*}:]
    \item \textbf{Calculate individual information content}:
    \begin{align*}
        I_H &= -\log_2(0.8) \approx 0.3219 \text{ bits} \\
        I_T &= -\log_2(0.2) \approx 2.3219 \text{ bits}
    \end{align*}
    
    \item \textbf{Calculate entropy as expected value}:
    \[
    H = 0.8 \times 0.3219 + 0.2 \times 2.3219 = 0.7219 \text{ bits}
    \]
    
    \item \textbf{Verify using direct formula}:
    \[
    H = -[0.8\log_2(0.8) + 0.2\log_2(0.2)] \approx 0.7219 \text{ bits}
    \]
\end{enumerate}

\textbf{Key Insights}:
\begin{itemize}
    \item \textbf{Extreme cases}:
    \begin{itemize}
        \item Fair coin (P=0.5): $H = 1.0$ bit (maximum uncertainty)
        \item Always heads (P=1.0): $H = 0$ bits (no uncertainty)
        \item 90\% heads: $H \approx 0.469$ bits
    \end{itemize}
\end{itemize}
\end{examplebox}

\subsubsection{Entropy of English Text: A Practical Case Study}

\begin{examplebox}
\textbf{Calculating English Letter Entropy}:

Based on letter frequencies in typical English text:
\[
H \approx 4.18 \text{ bits/letter}
\]

\textbf{Layered Interpretation}:
\begin{itemize}
    \item \textbf{First-order entropy (letters independent)}: 4.18 bits/letter
    \item \textbf{Actual uncertainty is lower}: Letters have dependencies (Q→U)
    
    \item \textbf{Comparison with encoding schemes}:
    \begin{center}
    \begin{tabular}{lcl}
    \toprule
    \textbf{Encoding Method} & \textbf{Bits/Letter} \\
    \midrule
    Naive (5 bits for 26 letters) & 5.00 \\
    Huffman (letter-based) & 4.30 \\
    Using digram frequencies & 3.90 \\
    Using word frequencies & 2.30 \\
    Optimal with full context & $\sim$1.50 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{itemize}
\end{examplebox}

\subsubsection{Beyond First-Order Entropy: The Full Picture}

\textbf{Higher-Order Entropies} quantify uncertainty while accounting for increasing context:

\begin{itemize}
    \item \textbf{Zero-order entropy ($H_0$)}: $H_0 = \log_2 |\mathcal{X}|$
    \item \textbf{First-order entropy ($H_1$)}: $H_1 = -\sum p(x)\log_2 p(x)$
    \item \textbf{Second-order entropy ($H_2$)}: $H_2 = -\sum p(x,y)\log_2 p(x|y)$
    \item \textbf{$N$th-order entropy ($H_N$)}: $H_N = -\sum p(x_1,\dots,x_N)\log_2 p(x_N \mid x_1,\dots,x_{N-1})$
\end{itemize}

\subsubsection{Entropy Rate}

The \textbf{entropy rate} of a source is defined as the limiting uncertainty per symbol when arbitrarily long contexts are available:
\[
H_\infty = \lim_{N \to \infty} H_N
\]

\subsubsection{The Entropy Theorem: Why It Matters}

\begin{definitionbox}
\textbf{Expected Code Length}

For a source with symbols $\{x_1, x_2, \dots, x_n\}$ having probabilities $\{p_1, p_2, \dots, p_n\}$, and a code that assigns codeword lengths $\{\ell_1, \ell_2, \dots, \ell_n\}$, the \textbf{expected code length} $L$ is:

\[
L = \mathbb{E}[\ell(X)] = \sum_{i=1}^n p_i \ell_i \quad \text{(bits per symbol)}
\]

This measures the average number of bits needed to encode one symbol from the source.
\end{definitionbox}

\begin{definitionbox}
\textbf{Compression Ratio and Efficiency}

For a source with entropy $H(X)$ and code with expected length $L$:
\begin{itemize}
    \item \textbf{Compression ratio}: $\rho = \frac{\text{original bits}}{\text{compressed bits}}$
    \item \textbf{Efficiency}: $\eta = \frac{H(X)}{L} \leq 1$
    \item \textbf{Redundancy}: $R = L - H(X) \geq 0$
\end{itemize}

Perfect compression occurs when $\eta = 1$ (100\% efficient) and $R = 0$.
\end{definitionbox}

\begin{importantbox}
\textbf{Shannon's Source Coding Theorem (1948)}

For a discrete memoryless source with entropy $H$ and any $\epsilon > 0$:

\begin{enumerate}
    \item \textbf{Converse (Impossibility Result)}: \\
    No lossless coding scheme can achieve expected code length $L < H$.
    \[
    \boxed{L \geq H \quad \text{for any uniquely decodable code}}
    \]
    
    \item \textbf{Achievability (Possibility Result)}: \\
    There exists a lossless coding scheme (specifically, block coding with sufficiently large block size $n$) such that:
    \[
    \boxed{H \leq L < H + \epsilon}
    \]
    Equivalently: For any $\epsilon > 0$, $\exists n$ such that:
    \[
    \frac{L_n}{n} < H + \epsilon
    \]
    where $L_n$ is the expected length for blocks of size $n$.
\end{enumerate}

\textbf{Interpretation}:
\begin{itemize}
    \item \textbf{Entropy is the fundamental limit}: $H$ bits/symbol is the best we can ever do
    \item \textbf{We can get arbitrarily close}: With clever coding, we can approach this limit as closely as desired
    \item \textbf{The gap is achievable}: The "+$\epsilon$" represents practical overhead that can be made arbitrarily small
\end{itemize}
\end{importantbox}

\begin{examplebox}
\textbf{Understanding the Theorem with Numbers}

Consider a binary source with $p(0)=0.9$, $p(1)=0.1$:
\[
H = -0.9\log_2 0.9 - 0.1\log_2 0.1 \approx 0.469 \text{ bits/symbol}
\]

\begin{itemize}
    \item \textbf{Naive coding}: Use 1 bit per symbol → $L = 1.0$, efficiency $\eta = 0.469/1.0 = 46.9\%$
    
    \item \textbf{Huffman coding}: 0→0, 1→1 (same as naive!) → $L = 1.0$, $\eta = 46.9\%$
    \textit{Why so bad?} Because we're coding symbols individually.
    
    \item \textbf{Block coding (n=2)}: Code pairs of symbols:
    \begin{align*}
        00 &\to 0 \quad (\ell=1, p=0.81) \\
        01 &\to 10 \quad (\ell=2, p=0.09) \\
        10 &\to 110 \quad (\ell=3, p=0.09) \\
        11 &\to 111 \quad (\ell=3, p=0.01)
    \end{align*}
    $L_2 = 0.81\times1 + 0.09\times2 + 0.09\times3 + 0.01\times3 = 1.29$ bits/block
    Per symbol: $L = L_2/2 = 0.645$ bits/symbol, $\eta = 0.469/0.645 \approx 72.7\%$
    
    \item \textbf{Block coding (n=3)}: Would get even closer to 0.469
    
    \item \textbf{Theoretical limit}: As $n \to \infty$, $L \to 0.469$
\end{itemize}

\textbf{Key insight}: The theorem tells us:
\begin{enumerate}
    \item We can never beat 0.469 bits/symbol (impossibility)
    \item We can get as close as we want to 0.469 bits/symbol (achievability)
\end{enumerate}
\end{examplebox}

\begin{importantbox}
\textbf{What the Theorem Does NOT Say}
\begin{itemize}
    \item \textbf{It doesn't say how to construct the code} - just that one exists
    \item \textbf{It doesn't guarantee practical implementation} - block size $n$ might need to be huge
    \item \textbf{It doesn't account for computational complexity} - the code might be too complex to implement
    \item \textbf{It assumes we know the true probabilities} - in practice, we estimate them
\end{itemize}

Yet, this theorem is revolutionary because it:
\begin{enumerate}
    \item Establishes a \textbf{fundamental limit} (like the speed of light in physics)
    \item Provides a \textbf{benchmark} for evaluating compression algorithms
    \item Guides algorithm design toward this limit
\end{enumerate}
\end{importantbox}

\subsubsection{Key Takeaways}

\begin{itemize}
    \item Entropy measures both \textbf{average information} and \textbf{uncertainty}
    \item Higher-order models reduce entropy by exploiting dependencies
    \item The entropy rate represents the ultimate compression limit
    \item Shannon's theorem precisely separates the \emph{possible} from the \emph{impossible}
\end{itemize}

\subsection{Entropy as a Lower Bound}

\subsubsection{The Fundamental Inequality}

\begin{theorem}[Entropy Lower Bound]
For any \textbf{uniquely decodable} code $C$ for source $X$:
\[
L(C) \geq H(X)
\]
where $L(C) = \mathbb{E}[\ell(X)] = \sum_i p_i \ell_i$ is the expected code length.
\end{theorem}

\begin{examplebox}
\textbf{Binary Source with $p(0)=0.9$, $p(1)=0.1$}
\[
H = -0.9\log_2 0.9 - 0.1\log_2 0.1 \approx 0.469 \text{ bits/symbol}
\]
\textbf{Why we can't achieve 0.4 bits/symbol}:
\begin{enumerate}
    \item For 100 symbols, typical sequences: $2^{100 \times 0.469} \approx 2^{46.9}$
    \item To encode all uniquely, need at least $2^{46.9}$ codewords
    \item At 0.4 bits/symbol, total bits = 40
    \item \# codewords $\leq 2^{40} < 2^{46.9}$ → impossible!
\end{enumerate}
\end{examplebox}

\subsection{Kraft-McMillan Inequality: Core Theoretical Tool}

\subsubsection{Statement and Interpretation}

\begin{theorem}[Kraft-McMillan Inequality (Binary Case)]
Let $\ell_1, \ell_2, \ldots, \ell_m$ be the lengths of codewords in a \textbf{prefix code}. Then:
\[
\sum_{i=1}^m 2^{-\ell_i} \leq 1
\]
Conversely, if integers $\ell_1, \ldots, \ell_m$ satisfy this inequality, then there exists a binary prefix code with these lengths.
\end{theorem}

\begin{examplebox}
\textbf{Tree Visualization of Kraft Inequality}

Consider a binary tree of depth $L = \max \ell_i$:
\begin{center}
\begin{tikzpicture}[scale=0.8]
% Level 0 (root)
\node[circle, draw] (root) at (0,0) {0};
% Level 1
\node[circle, draw] (l1) at (-2,-1) {0};
\node[circle, draw] (r1) at (2,-1) {1};
\draw (root) -- node[left, pos=0.3] {0} (l1);
\draw (root) -- node[right, pos=0.3] {1} (r1);

% Level 2 from left node
\node[circle, draw] (ll2) at (-3,-2) {00};
\node[circle, draw] (lr2) at (-1,-2) {01};
\draw (l1) -- node[left, pos=0.3] {0} (ll2);
\draw (l1) -- node[right, pos=0.3] {1} (lr2);

% Level 2 from right node
\node[circle, draw] (rl2) at (1,-2) {10};
\node[circle, draw] (rr2) at (3,-2) {11};
\draw (r1) -- node[left, pos=0.3] {0} (rl2);
\draw (r1) -- node[right, pos=0.3] {1} (rr2);

% Highlight a code assignment
\node[leaf node] (leaf1) at (-3,-3) {A: $\ell=2$};
\node[leaf node] (leaf2) at (-1,-3) {B: $\ell=3$};
\node[leaf node] (leaf3) at (1,-3) {C: $\ell=3$};
\draw[dashed, thick, red] (ll2) -- node[right] {} (leaf1);
\draw[dashed, thick, red] (lr2) -- node[right] {} (leaf2);
\draw[dashed, thick, red] (rl2) -- node[right] {} (leaf3);

% Leaves not used
\node at (3,-3) [draw, dashed, gray, minimum width=1.2cm, minimum height=0.8cm] {};
\end{tikzpicture}
\end{center}

\textbf{Calculating Kraft sum for $\{\ell_A=2, \ell_B=3, \ell_C=3\}$}:
\[
\sum 2^{-\ell_i} = 2^{-2} + 2^{-3} + 2^{-3} = 0.25 + 0.125 + 0.125 = 0.5 \leq 1
\]
\end{examplebox}

\begin{examplebox}
\textbf{Testing Code Feasibility}
\begin{enumerate}
    \item \textbf{Valid lengths}: $\{1, 2, 3, 3\}$
    \[
    \sum = 2^{-1} + 2^{-2} + 2^{-3} + 2^{-3} = 0.5 + 0.25 + 0.125 + 0.125 = 1.0 \quad \text{VALID}
    \]
    \item \textbf{Invalid lengths}: $\{1, 1, 2\}$
    \[
    \sum = 2^{-1} + 2^{-1} + 2^{-2} = 0.5 + 0.5 + 0.25 = 1.25 > 1 \quad \text{INVALID}
    \]
\end{enumerate}
\end{examplebox}

\subsection{Optimality of Huffman Codes (Theory Only)}

\subsubsection{The Optimality Theorem}

\begin{theorem}[Huffman Optimality]
Given a source with symbol probabilities $p_1, p_2, \ldots, p_m$, the Huffman algorithm produces a prefix code that \textbf{minimizes} the expected code length $L = \sum_{i=1}^m p_i \ell_i$ among all prefix codes.
\end{theorem}

\subsubsection{Relation to Entropy}

For any Huffman code:
\[
H(X) \leq L_{\text{Huffman}} < H(X) + 1
\]

\begin{examplebox}
\textbf{Understanding the "+1" Gap}

Consider source with probabilities $\{0.6, 0.3, 0.1\}$:
\[
H \approx 1.295 \text{ bits}
\]

\textbf{Ideal (non-integer) lengths}: $-\log_2 p_i = \{0.737, 1.737, 3.322\}$

\textbf{Huffman code}: 0.6→0, 0.3→10, 0.1→11
\[
L = 0.6\times1 + 0.3\times2 + 0.1\times2 = 1.4 \text{ bits}
\]

\textbf{Comparison}:
\begin{itemize}
    \item Entropy: 1.295 bits
    \item Huffman: 1.400 bits
    \item Gap: 0.105 bits (much less than 1!)
\end{itemize}
\end{examplebox}

\subsubsection{Why Huffman is Optimal but Not Perfect}

\begin{importantbox}
\textbf{Huffman is Optimal Within a Restricted Class}

Huffman is optimal among:
\begin{itemize}
    \item \textbf{Symbol-by-symbol} codes
    \item \textbf{Prefix} codes
    \item \textbf{Static} codes
\end{itemize}

But real optimality might require:
\begin{itemize}
    \item \textbf{Block coding}
    \item \textbf{Fractional bits} (arithmetic coding)
    \item \textbf{Adaptive probabilities}
\end{itemize}
\end{importantbox}

\subsection{Limitations of Symbol-by-Symbol Coding}

\subsubsection{Three Fundamental Limitations}

\begin{enumerate}
    \item \textbf{Cannot Exploit Dependencies}
    \item \textbf{Integer Length Constraint}: $\ell_i \in \mathbb{Z}^+$ but $-\log_2 p_i \in \mathbb{R}$
    \item \textbf{Memoryless Assumption}
\end{enumerate}

\begin{examplebox}
\textbf{English Text: The Cost of Symbol-by-Symbol}
\begin{itemize}
    \item \textbf{First-order entropy} (ignoring dependencies): ~4.0 bits/letter
    \item \textbf{Actual entropy rate} (with dependencies): ~1.5 bits/letter
    \item \textbf{Huffman on letters}: ~4.0 bits/letter
    \item \textbf{Gap}: 2.5 bits/letter wasted due to ignoring dependencies
\end{itemize}
\end{examplebox}

\subsection{Block Coding and Improved Efficiency}

\subsubsection{The Block Coding Idea}

Instead of coding symbols individually, group them into blocks of length $n$:
\[
\mathbf{X} = (X_1, X_2, \ldots, X_n)
\]

\begin{definitionbox}
\textbf{$n$th Extension of a Source}

For a source with alphabet $\mathcal{X}$, the $n$th extension has alphabet:
\[
\mathcal{X}^n = \{(x_1, \ldots, x_n) : x_i \in \mathcal{X}\}
\]
with size $|\mathcal{X}|^n$.
\end{definitionbox}

\subsubsection{Key Mathematical Results}

\begin{theorem}[Entropy of Block Source]
For a discrete memoryless source:
\[
H(X^n) = nH(X)
\]
\end{theorem}

\begin{theorem}[Block Coding Performance]
There exists a prefix code $C_n$ for $X^n$ such that:
\[
nH(X) \leq L_n < nH(X) + 1
\]
Dividing by $n$:
\[
H(X) \leq \frac{L_n}{n} < H(X) + \frac{1}{n}
\]
\end{theorem}

\begin{importantbox}
\textbf{The Magic of Block Coding}

As $n \to \infty$:
\[
\frac{L_n}{n} \to H(X)
\]
We can approach entropy \textbf{arbitrarily closely} by making blocks larger!
\end{importantbox}

\subsubsection{Step-by-Step Example}

\begin{examplebox}
\textbf{Binary Source: $p(0)=0.9$, $p(1)=0.1$, $H \approx 0.469$}

\textbf{Step 1: $n=1$ (symbol-by-symbol)}
\begin{itemize}
    \item Huffman: 0→0, 1→1
    \item $L_1 = 1$ bit/symbol
    \item Efficiency: $\eta = 0.469/1 = 46.9\%$
\end{itemize}

\textbf{Step 2: $n=2$ (code pairs)}
\begin{itemize}
    \item Block probabilities: $P(00)=0.81$, $P(01)=0.09$, $P(10)=0.09$, $P(11)=0.01$
    \item Codes: 00→0, 01→10, 10→110, 11→111
    \item Per symbol: $L_2/2 = 0.645$ bits/symbol
    \item Efficiency: $\eta = 0.469/0.645 = 72.7\%$
\end{itemize}
\end{examplebox}

\subsection{Trade-Offs in Block Coding}

\subsubsection{The Engineering Challenges}

\begin{enumerate}
    \item \textbf{Exponential Alphabet Growth}: $|\mathcal{X}^n| = |\mathcal{X}|^n$
    \item \textbf{Memory Requirements}: Huffman tree has $2m^n - 1$ nodes
    \item \textbf{Computational Complexity}: $O(m^n \log m^n)$
    \item \textbf{Delay and Latency}: Must wait for $n$ symbols
\end{enumerate}

\subsection{From Block Coding to Modern Compression}

\subsubsection{Arithmetic Coding: Fractional-Bit Block Coding}

\begin{importantbox}
\textbf{Arithmetic Coding as "Infinite Block Coding"}

Arithmetic coding cleverly avoids the exponential growth problem:
\begin{itemize}
    \item \textbf{Idea}: Encode entire message as a single real number in [0,1)
    \item \textbf{No explicit blocks}: Processes symbols sequentially
    \item \textbf{Fractional bits}: Achieves $L \approx H(X)$ without large $n$
    \item \textbf{Removes integer constraint}: No "+1" overhead!
\end{itemize}
\end{importantbox}

\subsubsection{Context Modeling: Approximating Large Blocks}

Instead of explicit block coding, modern compressors use:
\begin{enumerate}
    \item \textbf{Context Models}: Predict next symbol based on previous $k$ symbols
    \item \textbf{Prediction + Residual Coding}: Encode only prediction error
    \item \textbf{Dictionary Methods (LZ family)}: Build dictionary of previously seen phrases
\end{enumerate}

\subsection{What Theory Guarantees vs What Practice Achieves}

\begin{center}
\begin{tabularx}{\textwidth}{|l|X|X|}
\hline
\textbf{Aspect} & \textbf{Theory Guarantees} & \textbf{Practice Achieves} \\
\hline
\textbf{Optimality} & Can approach entropy arbitrarily closely & Gets close, but with practical limits \\
\hline
\textbf{Block Size} & $n \to \infty$ gives optimality & $n$ limited by memory, latency, complexity \\
\hline
\textbf{Complexity} & Ignored, infinite resources allowed & Critical constraint; often dominates design \\
\hline
\end{tabularx}
\end{center}

\subsection{Summary and Key Takeaways}

\begin{importantbox}
\textbf{Five Fundamental Lessons}
\end{importantbox}

\begin{enumerate}
    \item \textbf{Entropy is the Absolute Limit}: $L \geq H(X)$ for any lossless code
    \item \textbf{Kraft-McMillan Constrains All Codes}: $\sum 2^{-\ell_i} \leq 1$
    \item \textbf{Huffman is Optimal Among Prefix Codes}: But limited by integer lengths
    \item \textbf{Block Coding Allows Approaching Entropy}: $\lim_{n\to\infty} \frac{L_n}{n} = H(X)$
    \item \textbf{Practical Compression Balances Efficiency and Complexity}
\end{enumerate}

\subsubsection{The Big Picture}

\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw=black, thick, fill=lightblue, align=center, minimum width=2.5cm, minimum height=1cm}
]
\node[box] (entropy) {Entropy};
\node[box, right=of entropy] (kraft) {Kraft Inequality};
\node[box, right=of kraft] (huffman) {Huffman Coding};
\node[box, below=of kraft] (block) {Block Coding};
\node[box, right=of block] (practical) {Practical Methods};

\draw[->, thick] (entropy) -- node[above] {Lower bound} (kraft);
\draw[->, thick] (kraft) -- node[above] {Feasibility} (huffman);
\draw[->, thick] (huffman) -- node[right] {Integer limit} (block);
\draw[->, thick] (block) -- node[above] {Avoids cost} (practical);
\draw[->, thick, dashed] (entropy.south) |- node[pos=0.25, left] {Approach} (block.west);
\end{tikzpicture}
\end{center}

\subsubsection{Looking Forward}

\begin{itemize}
    \item \textbf{Next lecture: Arithmetic Coding}: Removes integer constraint
    \item \textbf{Then: Dictionary Methods (LZ family)}: Adaptive to data statistics
    \item \textbf{Finally: Modern Compressors}: Combining multiple techniques
\end{itemize}

\begin{center}
\fbox{\parbox{0.9\textwidth}{
\centering
\textbf{Final Thought}

Shannon's 1948 paper told us \emph{exactly how good compression could possibly be}. Every compressor since has been trying to approach that limit while staying within practical constraints.

The gap between theory and practice is where engineering creativity lives!
}}
\end{center}

