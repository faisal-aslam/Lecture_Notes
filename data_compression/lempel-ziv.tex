\section{Dictionary-Based Compression: The Lempel--Ziv Revolution}

\subsection{Motivation: Hitting the Limits of Statistical Coding}

Statistical coding methods such as Huffman and arithmetic coding achieve near-optimal compression \emph{when the source statistics are known or can be accurately estimated}. However, these methods fundamentally rely on estimating probabilities over symbols or blocks of symbols.

As discussed earlier, block-based statistical coding faces a fundamental dilemma.

\subsubsection{Recap: The Block Coding Dilemma}

Increasing block length allows a coder to better capture dependencies between symbols and approach the entropy rate of the source. However, this comes at a steep cost:
\begin{itemize}
    \item The number of possible blocks grows exponentially with block length.
    \item Estimating probabilities reliably requires exponentially more data.
    \item Memory and computational complexity quickly become impractical.
\end{itemize}

As a result, practical statistical coders are constrained to small contexts and local dependencies.

\subsubsection{The Promise of Exploiting Long-Range Repetition}

Real-world data---text, executable files, logs, DNA sequences---often contains \emph{long-range repetition}. Patterns may repeat far apart, well beyond the reach of fixed-size statistical contexts.

\begin{importantbox}
Statistical coding models \emph{how often} symbols occur, but does not directly model \emph{where long repeated patterns occur}.
\end{importantbox}

This observation motivates a radically different approach to compression.

\subsection{Paradigm Shift: From Statistics to Dictionaries}

Instead of estimating probabilities, dictionary-based compression learns the source \emph{by example}.

\subsubsection{Core Philosophy of Dictionary Coding}

Dictionary-based coders operate on a simple idea:
\begin{quote}
    \emph{Replace repeated substrings by references to earlier occurrences.}
\end{quote}

As the input is processed sequentially, a dictionary of previously seen substrings is constructed. Future occurrences are encoded by references into this dictionary.

\begin{importantbox}
You can think of Lempel--Ziv methods as \emph{learning the source structure rather than estimating probabilities}.
\end{importantbox}

Crucially, the encoder and decoder process the input in exactly the same order and therefore build identical dictionaries \emph{without transmitting the dictionary explicitly}.

\subsubsection{Explicit vs.\ Implicit (Adaptive) Dictionaries}

Dictionary-based methods fall into two categories:
\begin{itemize}
    \item \textbf{Implicit dictionaries}: The dictionary is defined by a sliding window of recent output (e.g., LZ77, LZSS).
    \item \textbf{Explicit dictionaries}: The dictionary is stored and indexed explicitly (e.g., LZ78).
\end{itemize}

\subsection{The Lempel-Ziv Algorithms}

\subsubsection{LZ77: The Sliding Window Algorithm}

Published in 1977 by Abraham Lempel and Jacob Ziv, LZ77 introduced the concept of using recent history as a dictionary. It forms the foundation for many practical compression algorithms including gzip, PNG, and ZIP.

\textbf{The Sliding Window Model:}
LZ77 maintains a sliding window divided into two parts:
\begin{itemize}
    \item \textbf{Search buffer}: Contains the $N$ most recently processed symbols (the "dictionary").
    \item \textbf{Look-ahead buffer}: Contains the next $L$ symbols to be encoded.
\end{itemize}

\textbf{The Encoding Tuple: (Offset, Length, Next Symbol):}
At each step, the encoder searches for the longest string in the search buffer that matches the beginning of the look-ahead buffer. It outputs a triple:
\[
(\text{offset}, \text{length}, \text{next symbol})
\]

\subsubsection{LZSS: Improving Practical Efficiency}

Published in 1982 by James Storer and Thomas Szymanski, LZSS addresses a key inefficiency in LZ77.

\textbf{Flag Bits: Literal vs. Match:}
LZSS introduces a \textbf{flag bit} preceding each encoded element:
\begin{itemize}
    \item \textbf{Flag = 0}: Next item is a literal symbol (8 bits for ASCII).
    \item \textbf{Flag = 1}: Next item is a match pair (offset, length).
\end{itemize}

\textbf{Match Threshold:}
LZSS introduces a \textbf{match threshold}: only encode matches that are \emph{long enough} to actually save bits.

\subsubsection{LZ78: The Dictionary Growth Algorithm}

Published in 1978, LZ78 takes a different approach: it builds an explicit dictionary that grows as encoding proceeds.

\textbf{Encoding Pairs: (Dictionary Index, New Symbol):}
LZ78 outputs pairs:
\[
(\text{index}, \text{symbol})
\]
where the new phrase (dictionary[index] + symbol) is added to the dictionary.

\subsubsection{LZW (Lempel-Ziv-Welch)}

LZW (1984) is a popular LZ78 variant used in GIF, TIFF, and UNIX compress.

\textbf{LZW Key Features:}
\begin{itemize}
    \item \textbf{No explicit next symbol}: Dictionary entries are added \emph{before} they're used.
    \item \textbf{Fixed-width codes}: Typically 12-bit codes (4096 dictionary entries).
    \item \textbf{Adaptive dictionary}: Starts with all single symbols (256 entries for bytes).
\end{itemize}

\subsection{Theoretical Properties: Why Lempel--Ziv Works}

\subsubsection{The Universality Principle}

Lempel--Ziv algorithms have a remarkable theoretical property:

\begin{definitionbox}
\textbf{Universal Compression}: An algorithm is universal if it can compress any stationary ergodic source to its entropy rate \emph{without prior knowledge} of the source statistics.
\end{definitionbox}

Both LZ77 and LZ78 are universal compressors.

\subsubsection{Asymptotic Optimality}

\begin{theorem}[Ziv \& Lempel, 1977-1978]
For any stationary ergodic source with entropy rate $H$, let $L_n$ be the length of the LZ77 or LZ78 encoding of $n$ source symbols. Then:
\[
\lim_{n \to \infty} \frac{\mathbb{E}[L_n]}{n} = H
\]
\end{theorem}

\subsection{Bridging Paradigms: Dictionary vs. Entropy Coding}

\subsubsection{Complementary Approaches}

Dictionary coding and entropy coding address different aspects of compression:
\begin{itemize}
    \item \textbf{Dictionary coding}: Exploits \emph{structural redundancy} (repetitions).
    \item \textbf{Entropy coding}: Exploits \emph{statistical redundancy} (skewed symbol frequencies).
\end{itemize}

\subsubsection{The Hybrid Approach}

Modern compressors combine both approaches. The DEFLATE algorithm (used in ZIP, PNG, gzip) works this way:
\begin{enumerate}
    \item LZSS finds repeated strings, outputs literals and matches.
    \item Huffman coding compresses:
    \begin{itemize}
        \item Literal symbols (0-255)
        \item Match lengths (257-285)
        \item Match distance codes (0-29)
    \end{itemize}
\end{enumerate}

\subsection{Summary and Forward Look}

\subsubsection{Key Takeaways}

\begin{itemize}
    \item \textbf{Paradigm shift}: Dictionary coding exploits repetition rather than probabilities.
    \item \textbf{LZ77}: Sliding window approach, encodes matches as triples.
    \item \textbf{LZSS}: Practical improvement with flag bits and match thresholds.
    \item \textbf{LZ78}: Explicit dictionary growth, encodes pairs.
    \item \textbf{Universality}: LZ methods work on any stationary source without prior knowledge.
    \item \textbf{Modern practice}: Hybrid systems (LZSS + Huffman) dominate.
\end{itemize}

% ================= END OF CHAPTER EXERCISES =================

\subsection*{End of Chapter Exercises}

\begin{exercisebox}
\textbf{Exercise 5.1 (LZ77 Encoding)}\\
Encode the string \texttt{TOBEORNOTTOBEORTOBEORNOT} using LZ77 with a search buffer of size 12. Show your work step by step with the search buffer, look-ahead buffer, and output tuples.
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.2 (LZSS Efficiency Analysis)}\\
Compare LZ77 and LZSS for encoding a sequence with parameters: 8-bit symbols, 12-bit offsets, 4-bit lengths.
\begin{enumerate}[label=(\alph*)]
    \item How many bits does LZ77 use for a non-match?
    \item How many bits does LZSS use for a literal?
    \item What is the minimum match length that saves bits in LZSS?
    \item When would LZ77 be more efficient than LZSS?
\end{enumerate}
\end{exercisebox}


\begin{exercisebox}
\textbf{Exercise 5.3 (LZ78 Dictionary Growth)}\\
Encode the string \texttt{ABABABABA} using LZ78. Show the dictionary after each step and the complete output sequence. How many dictionary entries are created?
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.4 (Algorithm Comparison)}\\
For each string below, predict which would perform better: LZ77/LZSS or LZ78? Explain why.
\begin{enumerate}[label=(\alph*)]
    \item \texttt{AAAAAAAAAAAAAAAAAAAA} (20 A's)
    \item \texttt{ABCDEFGHIJKLMNOPQRSTUVWXYZ}
    \item \texttt{ABABABABABABABABABAB}
    \item A file containing the same 1KB block repeated 100 times
\end{enumerate}
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.5 (Decoding Challenge)}\\
Decode the following LZSS encoded sequence (match threshold = 3):
\begin{verbatim}
0 T
0 H
0 E
1 (3,5)
0 _
0 _
1 (5,4)
0 .
\end{verbatim}
Where: 
- Each line represents one encoded element
- Flag 0 indicates a literal character
- Flag 1 indicates a match (offset, length)
- \_ represents space character
- . represents period

What is the original string? Show your decoding step by step.
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.6 (Window Size Analysis)}\\
Consider LZ77 with search buffer size $N$.
\begin{enumerate}[label=(\alph*)]
    \item How many bits are needed to encode an offset value?
    \item If we increase $N$ from 4096 to 65536, how many more bits are needed for offsets?
    \item What is the trade-off in choosing $N$?
    \item In practice, why might we choose $N = 32768$ over $N = 65536$ even if memory is available?
\end{enumerate}
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.7 (Universal Compression Intuition)}\\
Explain in your own words why Lempel--Ziv algorithms are universal. Why don't they need to know source statistics in advance? How do they "learn" the source structure?
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.8 (Hybrid Coding Design)}\\
Design a simple hybrid compressor that:
\begin{enumerate}[label=(\alph*)]
    \item Uses LZSS with match threshold 3
    \item Collects statistics on literals, lengths, and offsets
    \item Applies Huffman coding to each alphabet separately
\end{enumerate}
How would the decoder know which Huffman trees to use?
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.9 (Search Efficiency)}\\
A naive LZ77 implementation searches for matches by comparing the look-ahead buffer against every position in the search buffer.
\begin{enumerate}[label=(\alph*)]
    \item What is the time complexity of this approach?
    \item Describe how a hash table can improve search efficiency.
    \item What information would you store in the hash table?
    \item How would you handle hash collisions?
\end{enumerate}
\end{exercisebox}

\begin{exercisebox}
\textbf{Exercise 5.10 (Theoretical Limits)}\\
Prove or provide intuition for:
\begin{enumerate}[label=(\alph*)]
    \item LZ77 cannot achieve compression ratio better than the entropy rate for i.i.d. sources.
    \item LZ78 may perform poorly on very short inputs.
    \item For sources with long-range dependencies, LZ methods can outperform block Huffman coding even with large blocks.
\end{enumerate}
\end{exercisebox}
